#!/usr/bin/env python
import argparse, re, sys, math
from pathlib import Path
import pandas as pd
from bs4 import BeautifulSoup

# ---------- Config ----------
NFL_ALIASES = {
    "49ers": ["49ers","san francisco","sf","sfo","niners"],
    "Bears": ["bears","chicago","chi"],
    "Bengals": ["bengals","cincinnati","cin"],
    "Bills": ["bills","buffalo","buf"],
    "Broncos": ["broncos","denver","den"],
    "Browns": ["browns","cleveland","cle"],
    "Buccaneers": ["buccaneers","bucs","tampa bay","tb","tampa"],
    "Cardinals": ["cardinals","arizona","ari","cards"],
    "Chargers": ["chargers","los angeles chargers","la chargers","lac","san diego chargers","sd"],
    "Chiefs": ["chiefs","kansas city","kc"],
    "Colts": ["colts","indianapolis","ind"],
    "Commanders": ["commanders","washington","wsh","was","redskins"],
    "Cowboys": ["cowboys","dallas","dal"],
    "Dolphins": ["dolphins","miami","mia"],
    "Eagles": ["eagles","philadelphia","phi"],
    "Falcons": ["falcons","atlanta","atl"],
    "Giants": ["giants","new york giants","nyg"],
    "Jaguars": ["jaguars","jags","jacksonville","jax"],
    "Jets": ["jets","new york jets","nyj"],
    "Lions": ["lions","detroit","det"],
    "Packers": ["packers","green bay","gb"],
    "Panthers": ["panthers","carolina","car"],
    "Patriots": ["patriots","new england","ne","nwe"],
    "Raiders": ["raiders","las vegas","lv","oakland","oak"],
    "Rams": ["rams","los angeles rams","la rams","lar","st. louis rams","st louis rams"],
    "Ravens": ["ravens","baltimore","bal"],
    "Saints": ["saints","new orleans","no","nor"],
    "Seahawks": ["seahawks","seattle","sea"],
    "Steelers": ["steelers","pittsburgh","pit"],
    "Texans": ["texans","houston","hou"],
    "Titans": ["titans","tennessee","ten"],
    "Vikings": ["vikings","minnesota","min"],
}
ALIAS_TO_TEAM = {a: canon for canon, aliases in NFL_ALIASES.items() for a in aliases}

MATCH_RE = re.compile(
    r"(?P<a>[A-Za-z\.\s'\-]+?)\s+(?P<sep>at|@|vs\.?|v\.)\s+(?P<h>[A-Za-z\.\s'\-]+)", re.IGNORECASE
)
NUM_RE = re.compile(r"(-?\d+(?:\.\d+)?)")
SPREAD_RE = re.compile(r"(?:open(?:ing)?\s*)?(?:line|spread)[:\s]*([+-]?\d+(?:\.\d+)?)", re.IGNORECASE)
PK_RE = re.compile(r"\bP(?:K|ick)\b", re.IGNORECASE)

def _teamify(txt: str) -> str | None:
    t = (txt or "").strip().lower()
    hits = [ALIAS_TO_TEAM[a] for a in ALIAS_TO_TEAM if a in t]
    if len(set(hits)) == 1:
        return hits[0]
    # Try exact word match fallback
    for canon, aliases in NFL_ALIASES.items():
        for a in aliases:
            if re.search(rf"\b{re.escape(a)}\b", t):
                return canon
    return None

def _to_float(x):
    try:
        return float(x)
    except:
        return None

def _estimate_ml_from_spread(home_spread: float) -> tuple[float|None,float|None]:
    """
    Quick-&-dirty conversion of point spread -> moneyline.
    Uses a smoothed mapping similar to market-ish conversion; good enough for estimates.
    """
    if home_spread is None:
        return (None, None)
    s = abs(home_spread)
    # map spread to win prob with logistic-ish curve
    # tuned rough points: 2.5 -> ~60%, 3 -> ~61%, 6.5 -> ~70%, 10 -> ~79%
    p = 1/(1+math.exp(-(s-3.0)/1.6))
    p = 0.5 + (p-0.5) * 1.15  # widen a touch
    if home_spread < 0:  # home favored
        p_home = p
    else:
        p_home = 1-p
    # convert prob -> american ML
    def prob_to_ml(q):
        if q is None: return None
        q = max(min(q, 0.995), 0.005)
        if q >= 0.5:
            return -round(100*q/(1-q))
        else:
            return round(100*(1-q)/q)
    return prob_to_ml(p_home), prob_to_ml(1-p_home)

def _season_from_url(url: str) -> int | None:
    m = re.search(r"/(20\d{2})-", url)
    return int(m.group(1)) if m else None

def _week_from_context(txt: str) -> int | None:
    m = re.search(r"week\s+(\d{1,2})", txt, re.IGNORECASE)
    return int(m.group(1)) if m else None

def _rows_from_table(df: pd.DataFrame, season_hint: int | None) -> list[dict]:
    rows = []
    lower_cols = {c.lower(): c for c in df.columns}
    # Try common column names
    team_a = lower_cols.get("away") or lower_cols.get("team") or list(lower_cols.values())[0]
    team_h = lower_cols.get("home") or (list(lower_cols.values())[1] if len(lower_cols)>1 else None)
    spread_col = None
    for cand in ["open","opening","open spread","opening spread","spread","line"]:
        if cand in lower_cols:
            spread_col = lower_cols[cand]; break
    for _, r in df.iterrows():
        a = _teamify(str(r.get(team_a, "")))
        h = _teamify(str(r.get(team_h, ""))) if team_h else None
        if not a or not h: continue
        sp = _to_float(r.get(spread_col)) if spread_col else None
        # assume spread is fav side; convert to home perspective (negative home favored)
        if sp is not None and sp > 0:  # many tables list away as positive -> home negative
            sp = -sp
        rows.append({"season": season_hint, "away": a, "home": h, "spread_open": sp})
    return rows

def parse_url(url: str, estimate_ml: bool) -> list[dict]:
    import requests
    out = []
    season_hint = _season_from_url(url)
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    html = resp.text
    soup = BeautifulSoup(html, "html.parser")

    # 1) Try tables first
    try:
        tables = pd.read_html(html)
        for t in tables:
            out += _rows_from_table(t, season_hint)
    except Exception:
        pass

    # 2) Fallback: text scanning around matchup lines
    text = soup.get_text(" ", strip=True)
    blocks = re.split(r"\s{2,}", text)
    for blk in blocks:
        m = MATCH_RE.search(blk)
        if not m: continue
        a_raw, h_raw = m.group("a"), m.group("h")
        a = _teamify(a_raw); h = _teamify(h_raw)
        if not a or not h: continue

        # find a spread in the same block
        sp = None
        msp = SPREAD_RE.search(blk)
        if msp: sp = _to_float(msp.group(1))
        elif PK_RE.search(blk): sp = 0.0

        # If no direction given, assume pattern: "<Away> at <Home> ... <num>" where number is home spread negative if away listed first with '+'
        # Hard to be perfect; we’ll convert to home perspective if number looks away-leaning
        if sp is not None:
            # If appears with a '+' shortly after away team mention → away +X means home -X
            away_pos = blk.lower().find(a_raw.lower())
            plus_near_away = blk[away_pos:away_pos+40].find("+") if away_pos >= 0 else -1
            if plus_near_away != -1 and sp > 0:
                sp = -sp  # convert to home perspective
        out.append({"season": season_hint, "away": a, "home": h, "spread_open": sp})

    # Attach week where we can (from url/page text)
    page_txt = soup.get_text(" ", strip=True)
    week_hint = _week_from_context(page_txt) or _week_from_context(url) or None
    for r in out:
        r["week"] = week_hint

    # Estimate moneylines if asked
    if estimate_ml:
        for r in out:
            hml, aml = _estimate_ml_from_spread(r.get("spread_open"))
            r["ml_home_est"] = hml
            r["ml_away_est"] = aml

    # Final clean
    final = []
    for r in out:
        if not r.get("home") or not r.get("away"): continue
        final.append({
            "season": r.get("season"),
            "week": r.get("week"),
            "home": r["home"],
            "away": r["away"],
            "spread_open": r.get("spread_open"),
            "ml_home_est": r.get("ml_home_est"),
            "ml_away_est": r.get("ml_away_est"),
            "key_swha": f"{r.get('season')}|{r.get('week')}|{r['home']}|{r['away']}",
        })
    # drop dupes by (season, week, home, away)
    df = pd.DataFrame(final)
    if not df.empty:
        df = df.sort_values(["season","week","home","away"]).drop_duplicates(subset=["key_swha"])
    return df.to_dict("records")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--urls", nargs="+", required=True, help="One or more TheLines openers URLs")
    ap.add_argument("--estimate-ml", action="store_true", help="Estimate moneylines from spread")
    args = ap.parse_args()

    all_rows = []
    for u in args.urls:
        try:
            rows = parse_url(u, estimate_ml=args.estimate_ml)
            print(f"[ok] {u} rows={len(rows)}")
            all_rows += rows
        except Exception as e:
            print(f"[err] {u} -> {e}", file=sys.stderr)

    if not all_rows:
        print("[warn] No rows parsed.")
        sys.exit(0)

    out = pd.DataFrame(all_rows)
    out["season"] = pd.to_numeric(out["season"], errors="coerce").astype("Int64")
    out["week"]   = pd.to_numeric(out["week"], errors="coerce").astype("Int64")

    exports = Path("exports"); exports.mkdir(parents=True, exist_ok=True)
    dest = exports / "historical_odds_thelines.csv"

    # Merge with any existing file and dedupe
    if dest.exists():
        old = pd.read_csv(dest)
        out = pd.concat([old, out], ignore_index=True)

    out = out.drop_duplicates(subset=["key_swha"]).sort_values(["season","week","home","away"])
    out.to_csv(dest, index=False)
    print(f"[write] {dest} rows={len(out)}")

if __name__ == "__main__":
    main()
