import argparse, os, json, joblib
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.dummy import DummyClassifier

def load_edges(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
df.columns=[c.strip().lower() for c in df.columns]

# >>> ensure parlay-only & simple parlay features
import numpy as np
market_is_parlay = df.get("market", pd.Series(index=df.index)).astype(str).str.upper().eq("PARLAY")
legs_gt_one = pd.to_numeric(df.get("legs"), errors="coerce").fillna(1) > 1
has_dec_comb = df.get("dec_comb").notna() if "dec_comb" in df.columns else pd.Series(False, index=df.index)
df = df[market_is_parlay | legs_gt_one | has_dec_comb].copy()

def american_to_implied(series):
    o = pd.to_numeric(series, errors="coerce")
    p = pd.Series(np.nan, index=o.index)
    pos = o > 0
    p[pos]  = 100.0 / (o[pos] + 100.0)
    p[~pos] = (-o[~pos]) / ((-o[~pos]) + 100.0)
    return p

if "american" in df.columns and "p_parlay_naive" not in df.columns:
    df["p_parlay_naive"] = american_to_implied(df["american"])

for c in ["dec_comb","legs","parlay_stake"]:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

if "dec_comb" in df.columns and "log_dec_comb" not in df.columns:
    df["log_dec_comb"] = np.log(df["dec_comb"].clip(lower=1.000001))

if "legs" in df.columns and "log_legs" not in df.columns:
    df["log_legs"] = np.log1p(df["legs"].clip(lower=0))

if "p_parlay_naive" in df.columns and "logit_p_naive" not in df.columns:
    p = df["p_parlay_naive"].clip(1e-6, 1-1e-6)
    df["logit_p_naive"] = np.log(p/(1-p))
# <<< end
    
# >>> ensure parlay-only & simple parlay features
import numpy as np
market_is_parlay = df.get("market", pd.Series(index=df.index)).astype(str).str.upper().eq("PARLAY")
legs_gt_one = pd.to_numeric(df.get("legs"), errors="coerce").fillna(1) > 1
has_dec_comb = df.get("dec_comb").notna() if "dec_comb" in df.columns else pd.Series(False, index=df.index)
df = df[market_is_parlay | legs_gt_one | has_dec_comb].copy()

def american_to_implied(series):
    o = pd.to_numeric(series, errors="coerce")
    p = pd.Series(np.nan, index=o.index)
    pos = o > 0
    p[pos]  = 100.0 / (o[pos] + 100.0)
    p[~pos] = (-o[~pos]) / ((-o[~pos]) + 100.0)
    return p

if "american" in df.columns and "p_parlay_naive" not in df.columns:
    df["p_parlay_naive"] = american_to_implied(df["american"])

for c in ["dec_comb","legs","parlay_stake"]:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

if "dec_comb" in df.columns and "log_dec_comb" not in df.columns:
    df["log_dec_comb"] = np.log(df["dec_comb"].clip(lower=1.000001))

if "legs" in df.columns and "log_legs" not in df.columns:
    df["log_legs"] = np.log1p(df["legs"].clip(lower=0))

if "p_parlay_naive" in df.columns and "logit_p_naive" not in df.columns:
    p = df["p_parlay_naive"].clip(1e-6, 1-1e-6)
    df["logit_p_naive"] = np.log(p/(1-p))
# <<< end
df.columns = [c.strip().lower() for c in df.columns]
    return df

def coerce_numeric(df: pd.DataFrame, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
        else:
            df[c] = np.nan
    return df

def normalize_result_series(s: pd.Series) -> pd.Series:
    s = s.astype(str).str.strip().str.lower()
    map_win  = {"win":1,"w":1,"1":1,"true":1,"t":1,"yes":1,"y":1}
    map_lose = {"lose":0,"l":0,"0":0,"false":0,"f":0,"no":0,"n":0}
    return s.map({**map_win, **map_lose})

def implied_prob_from_american(odds):
    try:
        o = float(odds)
    except:
        return np.nan
    if np.isnan(o): return np.nan
    if o > 0: return 100.0 / (o + 100.0)
    if o < 0: return abs(o) / (abs(o) + 100.0)
    return np.nan

def make_time_split(df: pd.DataFrame):
    if "season" in df.columns:
        seasons = pd.to_numeric(df["season"], errors="coerce").dropna().unique()
        if len(seasons) >= 2:
            last = np.nanmax(seasons)
            tr = pd.to_numeric(df["season"], errors="coerce") < last
            va = pd.to_numeric(df["season"], errors="coerce") == last
            if tr.any() and va.any():
                return tr, va
    n = len(df)
    cut = max(1, int(n*0.8))
    tr = pd.Series([True]*cut + [False]*(n-cut), index=df.index)
    va = ~tr
    return tr, va

def fmt_auc(x):
    try:
        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):
            return "nan"
        return f"{float(x):.3f}"
    except Exception:
        return "nan"

def main():
    ap = argparse.ArgumentParser(description="Train parlay propensity model from graded edges.")
    ap.add_argument("--edges", required=True)
    ap.add_argument("--out", required=True, help="Output dir for model artifacts")
    ap.add_argument("--min-season", type=int, default=2017)
    args = ap.parse_args()

    os.makedirs(args.out, exist_ok=True)

    df = load_edges(args.edges)
    if "season" in df.columns:
        df["season"] = pd.to_numeric(df["season"], errors="coerce")
        df = df[df["season"] >= args.min_season].copy()
    if df.empty:
        raise SystemExit("No rows after season filter.")

    if "result" not in df.columns:
        raise SystemExit("No 'result' column found.")
    y_series = normalize_result_series(df["result"])
    mask_valid = y_series.isin([0,1])
    df = df[mask_valid].copy()
    y = y_series[mask_valid].astype(int)
    if df.empty:
        raise SystemExit("No trainable rows.")

    # ---- features ----
    num_cols = ["odds","line","p_win","dec_comb","legs","parlay_stake","stake","week_num","edge"]
    cat_cols = ["market","ref","sport","league"]

    # derived "edge"
    if "p_win" in df.columns and "odds" in df.columns:
        df["implied_prob"] = df["odds"].apply(implied_prob_from_american)
        df["edge"] = df["p_win"] - df["implied_prob"]
    else:
        df["edge"] = np.nan

    df = coerce_numeric(df, num_cols)
    for c in cat_cols:
        if c not in df.columns: df[c] = ""

    # drop all-NaN numeric columns
    num_cols_used = [c for c in num_cols if df[c].notna().any()]
    if set(num_cols_used) != set(num_cols):
        missing_all = sorted(set(num_cols) - set(num_cols_used))
        print(f"[train_parlay] Dropping all-NaN numeric columns: {missing_all}")
    num_cols = num_cols_used

    X = df[num_cols + cat_cols].copy()

    # preprocess
    num_pipe = Pipeline([
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler())
    ])
    cat_pipe = Pipeline([
        ("impute", SimpleImputer(strategy="most_frequent")),
        ("oh", OneHotEncoder(handle_unknown="ignore", min_frequency=5))
    ])
    pre = ColumnTransformer([
        ("num", num_pipe, num_cols),
        ("cat", cat_pipe, cat_cols)
    ])

    classes = np.unique(y)
    if len(classes) < 2:
        print(f"[train_parlay] One-class labels {classes}, using DummyClassifier.")
        pipe = Pipeline([("pre", pre), ("clf", DummyClassifier(strategy="prior"))])
        pipe.fit(X, y)
        auc_train = None
        auc_valid = None
        chosen_C = None
    else:
        tr_idx, va_idx = make_time_split(df)
        X_tr, y_tr = X[tr_idx], y[tr_idx]
        X_va, y_va = X[va_idx], y[va_idx]

        best_auc, best_C, best_pipe = float("-inf"), None, None
        for C in [0.01,0.1,0.25,0.5,1,2,4,8,16]:
            clf = LogisticRegression(max_iter=1000, solver="lbfgs", class_weight="balanced", C=C)
            pipe_try = Pipeline([("pre", pre), ("clf", clf)])
            pipe_try.fit(X_tr, y_tr)
            try:
                prob = pipe_try.predict_proba(X_va)[:,1]
                auc = roc_auc_score(y_va, prob)
            except Exception:
                auc = float("-inf")
            if auc > best_auc:
                best_auc, best_C, best_pipe = auc, C, pipe_try

        pipe = best_pipe
        chosen_C = best_C
        try:
            auc_train = roc_auc_score(y_tr, pipe.predict_proba(X_tr)[:,1]) if len(set(y_tr))>1 else None
        except Exception:
            auc_train = None
        try:
            auc_valid = roc_auc_score(y_va, pipe.predict_proba(X_va)[:,1]) if len(set(y_va))>1 else None
        except Exception:
            auc_valid = None
        print(f"[train_parlay] Selected C={chosen_C}, train AUC={fmt_auc(auc_train)}, valid AUC={fmt_auc(auc_valid)}")

    # save artifacts
    joblib.dump(pipe, os.path.join(args.out, "parlay_model.joblib"))
    meta = {
        "rows": int(len(df)),
        "win_rate": float(y.mean()) if len(y) else 0.0,
        "auc_train": float(auc_train) if isinstance(auc_train, (int,float)) and not (auc_train is None or np.isnan(auc_train)) else None,
        "auc_valid": float(auc_valid) if isinstance(auc_valid, (int,float)) and not (auc_valid is None or np.isnan(auc_valid)) else None,
        "chosen_C": chosen_C,
        "num_cols": num_cols,
        "cat_cols": cat_cols
    }
    with open(os.path.join(args.out,"meta.json"),"w",encoding="utf-8") as f:
        json.dump(meta,f,indent=2)
    print(f"Saved model to {args.out}\\parlay_model.joblib, rows={meta['rows']}, train AUC={fmt_auc(auc_train)}, valid AUC={fmt_auc(auc_valid)}")

if __name__ == "__main__":
    main()
# >>> ensure parlay-only if weâ€™re building a parlay model
market_is_parlay = df.get("market", pd.Series(index=df.index)).astype(str).str.upper().eq("PARLAY")
legs_gt_one = pd.to_numeric(df.get("legs"), errors="coerce").fillna(1) > 1
has_dec_comb = df.get("dec_comb").notna() if "dec_comb" in df.columns else pd.Series(False, index=df.index)
df = df[market_is_parlay | legs_gt_one | has_dec_comb].copy()
# <<< end



