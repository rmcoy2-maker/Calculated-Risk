from __future__ import annotations
import re
from pathlib import Path
import numpy as np
import pandas as pd
import streamlit as st

# Tolerate missing find_exports_dir
try:
    from lib.io_paths import find_exports_dir
except Exception:
    def find_exports_dir() -> Path:
        for cand in [
            Path("exports"),
            Path.cwd() / "exports",
            Path.stem and Path(__file__).parent.parent.parent / "exports",
        ]:
            try:
                if cand and Path(cand).exists():
                    return Path(cand)
            except Exception:
                pass
        return Path.cwd()

try:
    from lib.enrich import add_probs_and_ev, attach_teams
except Exception:
    def attach_teams(df: pd.DataFrame) -> pd.DataFrame: return df
    def add_probs_and_ev(df: pd.DataFrame) -> pd.DataFrame: return df

st.set_page_config(page_title="Compare Models", page_icon="ðŸ§ ", layout="wide")
st.title("Compare Models")

exp = find_exports_dir()

# ---- discover candidate files ----
cands: list[Path] = []

def add_if_exists(p: Path):
    try:
        if p.exists() and p.stat().st_size > 0 and p.suffix.lower() == ".csv":
            cands.append(p)
    except Exception:
        pass

for name in ["edges_graded_full.csv", "edges_graded_plus.csv", "edges_graded.csv", "edges.csv"]:
    add_if_exists(exp / name)

for p in exp.glob("edges_graded*_*.csv"):
    add_if_exists(p)

models_dir = exp / "models"
if models_dir.exists():
    for p in models_dir.rglob("*.csv"):
        add_if_exists(p)

if not cands:
    st.warning("No graded CSVs found in exports/. Drop per-model graded CSVs into exports/ or exports/models/<name>/.")
    st.stop()

labels = [f"{p.name} â€” {p.parent}" for p in cands]
sel = st.multiselect(
    "Select 2â€“6 graded files to compare (one per model)",
    options=list(range(len(cands))),
    format_func=lambda i: labels[i],
    default=list(range(min(2, len(cands))))
)
if not sel:
    st.info("Pick at least one file to compare.")
    st.stop()

@st.cache_data(ttl=60, show_spinner=False)
def load_and_enrich(path_str: str) -> pd.DataFrame:
    p = Path(path_str)
    df = pd.read_csv(p, low_memory=False, encoding="utf-8-sig")
    df = add_probs_and_ev(attach_teams(df))
    m = re.search(r"__(.+?)\.csv$", p.name)
    model = m.group(1) if m else (p.parent.name if p.parent.name not in ("exports", "models") else p.stem)
    df["__source"] = p.name
    df["__model"] = model
    return df

dfs = [load_and_enrich(str(cands[i])) for i in sel if cands[i].exists() and cands[i].stat().st_size > 0]
dfs = [d for d in dfs if not d.empty]
if not dfs:
    st.warning("Selected files are empty after load.")
    st.stop()

def ev_metrics(df: pd.DataFrame) -> dict:
    p = pd.to_numeric(df.get("p_model_use", np.nan), errors="coerce")
    ev = pd.to_numeric(df.get("ev_$1", np.nan), errors="coerce")
    return {
        "rows": len(df),
        "with_p": int(p.notna().sum()),
        "avg_p": float(np.nanmean(p)) if p.notna().any() else np.nan,
        "with_ev": int(ev.notna().sum()),
        "avg_ev_$1": float(np.nanmean(ev)) if ev.notna().any() else np.nan,
        "pos_ev_rows": int((ev >= 0).sum()),
    }

rows = []
for d in dfs:
    m = d["__model"].iloc[0]
    src = d["__source"].iloc[0]
    met = ev_metrics(d)
    met["model"] = m
    met["source"] = src
    rows.append(met)

tbl = pd.DataFrame(rows)[["model","source","rows","with_p","avg_p","with_ev","avg_ev_$1","pos_ev_rows"]]
st.subheader("EV-based comparison")
st.dataframe(tbl, use_container_width=True)

st.subheader("Top 25 edges per model")
for d in dfs:
    m = d["__model"].iloc[0]
    st.markdown(f"**{m}** â€” {d['__source'].iloc[0]}")
    show_cols = [c for c in ["game_id","home_team","away_team","market","selection","book","odds","p_model_use","computer_fair_odds","ev_$1"] if c in d.columns]
    st.dataframe(d.sort_values("ev_$1", ascending=False)[show_cols].head(25), use_container_width=True)

with st.expander("Tips", expanded=False):
    st.markdown("""
- Compare **graded** CSVs (with model probabilities). To create per-model files, export one CSV per model (e.g., `edges_graded_plus__game_lr_v3.csv`).
- Place them in `exports/` or `exports/models/<model_name>/`.
- This page compares **expected value** across files. For realized ROI, run Backtest on each file.
""")

