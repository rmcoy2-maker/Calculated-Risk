from __future__ import annotations
# -*- coding: utf-8 -*-
"""
03_Backtest.py â€” clean version
- Seasons/Weeks filters
- Parlay mode (optional) with graceful fallback if no grouping column
- Equity curve & summary metrics
- Near-miss parlays table
"""

import os
from pathlib import Path
from typing import Iterable, Tuple, Optional
import typing

import numpy as np
import pandas as pd
import streamlit as st

# -------------------- Global config --------------------
MIN_SEASON = 1966
VALID_WEEKS = list(range(1, 23))  # 1..22

PARLAY_GROUP_CANDIDATES = ["parlay_id", "slip_id", "ticket_id", "bet_id", "wager_id", "group_id"]
TS_CANDIDATES = ["sort_ts", "ts", "placed_at", "event_ts", "game_ts", "kickoff", "datetime", "date", "start_time", "scheduled"]

st.set_page_config(page_title="Backtest", layout="wide")

# -------------------- Small helpers --------------------
def am_to_dec(american) -> Optional[float]:
    try:
        a = float(american)
    except Exception:
        return None
    if a == 0:
        return None
    return 1.0 + (a / 100.0 if a > 0 else 100.0 / abs(a))

def compute_row_profit(row: pd.Series, use_parlay_first: bool = True) -> Tuple[float, float]:
    """Return (stake_used, profit_units) for a single leg row."""
    stake = None
    if use_parlay_first and not pd.isna(row.get("parlay_stake")):
        stake = float(row.get("parlay_stake"))
    elif not pd.isna(row.get("stake")):
        stake = float(row.get("stake"))
    else:
        stake = 1.0
    if stake is None or stake <= 0:
        stake = 1.0

    dec = am_to_dec(row.get("odds"))
    res = str(row.get("result") or "").lower()

    if dec is None or pd.isna(dec):
        return stake, 0.0
    if res == "win":
        return stake, stake * (dec - 1.0)
    if res == "lose":
        return stake, -stake
    if res == "push":
        return stake, 0.0
    return stake, 0.0  # unknown/ungraded

def _combined_decimal_odds(group: pd.DataFrame) -> Optional[float]:
    decs = []
    for _, r in group.iterrows():
        d = am_to_dec(r.get("odds"))
        if d is None or pd.isna(d):
            return None
        decs.append(float(d))
    if not decs:
        return None
    prod = 1.0
    for d in decs:
        prod *= d
    return prod

def _parlay_result(group: pd.DataFrame) -> str:
    # if any lose -> lose; if all win -> win; else if any push -> push; else unknown
    res = group["result"].astype(str).str.lower().fillna("")
    if (res == "lose").any():
        return "lose"
    if (res == "win").all() and len(res) > 0:
        return "win"
    if (res == "push").any():
        return "push"
    return ""

def compute_parlay_row_profit(row: pd.Series) -> Tuple[float, float]:
    """PnL for a single parlay row (already collapsed)."""
    stake = float(row.get("parlay_stake") or 1.0)
    dec   = row.get("dec_comb")
    res   = str(row.get("result") or "").lower()
    if dec is None or pd.isna(dec):
        return stake, 0.0
    if res == "win":
        return stake, stake * (float(dec) - 1.0)
    if res == "lose":
        return stake, -stake
    if res == "push":
        return stake, 0.0
    return stake, 0.0

# -------------------- IO helpers --------------------
def _find_edges_csv() -> Path:
    # 1) env override
    env = os.environ.get("EDGES_CSV")
    if env:
        p = Path(env)
        if p.exists():
            return p
    # 2) walk upward from this file until we find exports/edges.csv
    here = Path(__file__).resolve()
    for base in [here.parent, *here.parents]:
        candidate = base / "exports" / "edges.csv"
        if candidate.exists():
            return candidate
    # 3) last-ditch guess: repo-root/exports/edges.csv
    return here.parents[3] / "exports" / "edges.csv"

EDGES_CSV = _find_edges_csv()

def read_csv_safe(path: Path | str) -> pd.DataFrame:
    p = Path(path)
    if not p.exists() or p.stat().st_size == 0:
        return pd.DataFrame()
    for kw in ({"encoding": "utf-8-sig"}, {}, {"engine": "python"}):
        try:
            return pd.read_csv(p, **kw)
        except Exception:
            continue
    return pd.DataFrame()

# -------------------- Data normalization --------------------
def load_edges() -> pd.DataFrame:
    df = read_csv_safe(EDGES_CSV)
    if df.empty:
        return df
    df = df.copy()
    df.columns = [str(c).strip().lower() for c in df.columns]

    need = [
        "ts", "sort_ts", "sport", "league", "season", "week", "market", "ref",
        "side", "line", "odds", "p_win", "ev", "result", "stake", "parlay_stake", "game_id"
    ]
    for c in need:
        if c not in df.columns:
            df[c] = None

    df["sort_ts"] = pd.to_datetime(df["sort_ts"], errors="coerce")
    # If sort_ts is NA but ts exists, coerce ts
    if df["sort_ts"].isna().all() and "ts" in df.columns:
        df["sort_ts"] = pd.to_datetime(df["ts"], errors="coerce")

    df["season"] = pd.to_numeric(df["season"], errors="coerce").astype("Int64")
    df["week"]   = pd.to_numeric(df["week"], errors="coerce").astype("Int64")
    df["odds"]   = pd.to_numeric(df["odds"], errors="coerce")
    df["p_win"]  = pd.to_numeric(df["p_win"], errors="coerce")
    df["ev"]     = pd.to_numeric(df["ev"], errors="coerce")
    df["stake"]  = pd.to_numeric(df["stake"], errors="coerce")
    df["parlay_stake"] = pd.to_numeric(df["parlay_stake"], errors="coerce")
    df["result"] = df["result"].astype(str).str.lower().where(df["result"].notna(), None)
    return df

def seasons_from_df(df: pd.DataFrame) -> list[int]:
    if df.empty or "season" not in df.columns:
        return []
    vals = pd.to_numeric(df["season"], errors="coerce").dropna().astype(int).tolist()
    return sorted({s for s in vals if s >= MIN_SEASON})

def weeks_from_df(df: pd.DataFrame, seasons: list[int]) -> list[int]:
    if df.empty or "week" not in df.columns:
        return VALID_WEEKS
    if seasons:
        df = df[df["season"].isin(seasons)]
    vals = pd.to_numeric(df["week"], errors="coerce").dropna().astype(int).tolist()
    ws = sorted({w for w in vals if w in VALID_WEEKS})
    return ws or VALID_WEEKS

def ensure_sort_ts(df: pd.DataFrame) -> pd.DataFrame:
    """Create df['sort_ts'] reliably; never explode on missing columns."""
    if df is None or df.empty:
        return df
    if "sort_ts" in df.columns and pd.to_datetime(df["sort_ts"], errors="coerce").notna().any():
        # Ensure it's datetime
        out = df.copy()
        out["sort_ts"] = pd.to_datetime(out["sort_ts"], errors="coerce")
        return out

    s = pd.Series(pd.NaT, index=df.index, dtype="datetime64[ns]")
    for c in TS_CANDIDATES:
        if c in df.columns:
            s = s.fillna(pd.to_datetime(df[c], errors="coerce"))
    out = df.copy()
    if s.notna().any():
        base = pd.Timestamp.utcnow()
        out["sort_ts"] = s.fillna(base) + pd.to_timedelta(np.arange(len(out)), unit="s")
        return out

    # last resort: synthesize by row order
    base = pd.Timestamp.utcnow()
    out["sort_ts"] = base + pd.to_timedelta(np.arange(len(out)), unit="s")
    return out

def ensure_parlay_grouping(df: pd.DataFrame, parlay_mode: bool) -> Tuple[pd.DataFrame, Optional[str], bool]:
    """Return (df, group_col, parlay_mode). Make a safe grouping if none exists (one row per slip)."""
    if df is None or df.empty:
        return df, None, False
    for c in PARLAY_GROUP_CANDIDATES:
        if c in df.columns and df[c].notna().any():
            return df, c, parlay_mode
    if parlay_mode:
        out = df.copy()
        out["parlay_id"] = np.arange(len(out))  # degrade: each row its own group
        return out, "parlay_id", parlay_mode
    return df, None, parlay_mode

def legs_to_parlays(df: pd.DataFrame) -> pd.DataFrame:
    """Collapse leg rows into one parlay row per parlay_id with combined odds and single stake."""
    if df is None or df.empty:
        return pd.DataFrame()
    key = None
    for k in ("parlay_id","slip_id","ticket_id","bet_id"):
        if k in df.columns:
            key = k
            break
    if key is None:
        return pd.DataFrame()

    groups = []
    for pid, g in df.groupby(key, dropna=False):
        pstake = pd.to_numeric(g.get("parlay_stake"), errors="coerce").dropna()
        stake = float(pstake.iloc[0]) if not pstake.empty else float(
            pd.to_numeric(g.get("stake"), errors="coerce").dropna().iloc[0] if pd.to_numeric(g.get("stake"), errors="coerce").notna().any() else 1.0
        )
        dec = _combined_decimal_odds(g)
        result = _parlay_result(g)
        sort_ts = pd.to_datetime(g["sort_ts"], errors="coerce").min()
        season  = pd.to_numeric(g.get("season"), errors="coerce").dropna().astype(int)
        week    = pd.to_numeric(g.get("week"), errors="coerce").dropna().astype(int)
        season  = int(season.iloc[0]) if not season.empty else None
        week    = int(week.iloc[0]) if not week.empty else None

        groups.append(dict(
            sort_ts=sort_ts, season=season, week=week,
            market="PARLAY", ref=str(pid),
            odds=None, dec_comb=dec, parlay_stake=stake, result=result, legs=len(g),
            stake=None, p_win=None, ev=None, ts=sort_ts
        ))
    return pd.DataFrame(groups)

# -------------------- Analytics --------------------
def equity_curve(work: pd.DataFrame, starting_bankroll: float = 100.0, use_parlay_first: bool = True) -> pd.DataFrame:
    if work is None or work.empty:
        return pd.DataFrame({"ts": [], "equity": []})
    W = work.sort_values("sort_ts", ascending=True, na_position="last").copy()
    running = float(starting_bankroll)
    eq = []
    for _, r in W.iterrows():
        stake, pnl = compute_parlay_row_profit(r) if st.session_state.get("bk_mode_parlay", False) \
            else compute_row_profit(r, use_parlay_first=use_parlay_first)
        running += pnl
        eq.append(running)
    return pd.DataFrame({"ts": W["sort_ts"].values, "equity": eq})

def max_drawdown(equity_series: Iterable[float]) -> Tuple[float, float, float]:
    max_dd = 0.0
    peak = -np.inf
    trough = -np.inf
    run_peak = -np.inf
    for v in equity_series:
        run_peak = max(run_peak, v)
        dd = run_peak - v
        if dd > max_dd:
            max_dd = dd
            peak = run_peak
            trough = v
    return float(max_dd), float(peak if peak != -np.inf else 0.0), float(trough if trough != -np.inf else 0.0)

def summarize(work: pd.DataFrame, use_parlay_first: bool = True) -> dict:
    if work is None or len(work) == 0:
        return dict(total_bets=0, wins=0, losses=0, pushes=0, win_pct=0.0,
                    avg_ev=np.nan, total_stake=0.0, total_profit=0.0, roi=0.0,
                    max_drawdown=0.0, peak_equity=0.0, final_equity=0.0)

    if "result" not in work.columns:
        work = work.assign(result=np.nan)
    if "sort_ts" not in work.columns:
        work = work.assign(sort_ts=pd.NaT)

    res = work["result"].astype(str).str.lower()
    total_bets = len(work)
    wins   = int((res == "win").sum())
    losses = int((res == "lose").sum())
    pushes = int((res == "push").sum())

    stakes, profits = [], []
    for _, r in work.iterrows():
        s, p = (compute_parlay_row_profit(r) if st.session_state.get("bk_mode_parlay", False)
                else compute_row_profit(r, use_parlay_first=use_parlay_first))
        stakes.append(float(s)); profits.append(float(p))

    total_stake  = float(sum(stakes)) if stakes else 0.0
    total_profit = float(sum(profits)) if profits else 0.0
    win_pct = (wins / total_bets) if total_bets else 0.0
    roi     = (total_profit / total_stake) if total_stake > 0 else 0.0
    avg_ev  = float(pd.to_numeric(work.get("ev"), errors="coerce").dropna().mean()) if "ev" in work.columns else np.nan

    eq = equity_curve(work, starting_bankroll=0.0, use_parlay_first=use_parlay_first)
    max_dd, peak_eq, final_eq = 0.0, 0.0, 0.0
    if not eq.empty:
        md, pk, _ = max_drawdown(eq["equity"].tolist())
        max_dd, peak_eq = float(md), float(pk)
        final_eq = float(eq["equity"].iloc[-1])

    return dict(
        total_bets=total_bets, wins=wins, losses=losses, pushes=pushes,
        win_pct=win_pct, avg_ev=avg_ev,
        total_stake=total_stake, total_profit=total_profit, roi=roi,
        max_drawdown=max_dd, peak_equity=peak_eq, final_equity=final_eq,
    )

def build_near_misses(df: pd.DataFrame) -> pd.DataFrame:
    """Parlays with exactly one losing leg (requires leg-level rows with parlay grouping + result)."""
    if df is None or df.empty:
        return pd.DataFrame()
    key = None
    for k in ("parlay_id","slip_id","ticket_id","bet_id"):
        if k in df.columns:
            key = k
            break
    if key is None or "result" not in df.columns:
        return pd.DataFrame()

    out = []
    for pid, g in df.groupby(key, dropna=False):
        res_col = g["result"].astype(str).str.lower().fillna("")
        loses = int((res_col == "lose").sum())
        wins  = int((res_col == "win").sum())
        total = int(len(g))
        if total >= 2 and loses == 1 and wins >= 1:
            first_ts = pd.to_datetime(g.get("sort_ts"), errors="coerce").min()
            season   = pd.to_numeric(g.get("season"), errors="coerce").dropna().astype(int)
            week     = pd.to_numeric(g.get("week"), errors="coerce").dropna().astype(int)
            out.append(dict(
                parlay_id=str(pid),
                legs=total, lost_legs=loses, won_legs=wins,
                season=(int(season.iloc[0]) if not season.empty else None),
                week=(int(week.iloc[0]) if not week.empty else None),
                sort_ts=first_ts
            ))
    if not out:
        return pd.DataFrame()
    return pd.DataFrame(out).sort_values("sort_ts", ascending=False, na_position="last")

# -------------------- UI --------------------
st.title("Backtest")
st.caption(f"Reading edges from: `{EDGES_CSV}`")

edges = load_edges()
if edges.empty:
    st.info("No edges found in exports/edges.csv")
    st.stop()

# Season/Week controls (use edges as truth if no scores provided)
avail_seasons = seasons_from_df(edges) or [MIN_SEASON]
max_season = max(avail_seasons) if avail_seasons else MIN_SEASON
st.caption(f"Data window: **{MIN_SEASON}â€“{max_season}**")

st.session_state.setdefault("bk_start_bankroll", 100.0)
st.session_state.setdefault("bk_newest_first", True)
st.session_state.setdefault("bk_weeks", [])
st.session_state.setdefault("bk_use_parlay_first", True)
st.session_state.setdefault("bk_mode_parlay", False)

# Render Parlay toggle ONCE
st.toggle("Parlay mode", value=st.session_state["bk_mode_parlay"], key="bk_mode_parlay")

colA, colB, colC, colD = st.columns([2.2, 2.8, 1.2, 1.6])

with colA:
    st.caption("Seasons")
    season_labels = ["All"] + [str(s) for s in avail_seasons]
    selected = st.multiselect("Select seasons", season_labels, default=["All"], label_visibility="collapsed")
    selected_seasons = avail_seasons if ("All" in selected or not selected) else [int(s) for s in selected]

with colB:
    st.caption("Weeks (1â€“22)")
    wk_opts = weeks_from_df(edges, selected_seasons)
    weeks_pick = st.multiselect("Select weeks", wk_opts, default=wk_opts, label_visibility="collapsed")
    c1, c2 = st.columns(2)
    with c1:
        if st.button("All weeks", use_container_width=True):
            weeks_pick = wk_opts
    with c2:
        if st.button("Clear weeks", use_container_width=True):
            weeks_pick = []
    st.session_state["bk_weeks"] = weeks_pick

with colC:
    st.caption("Starting bankroll (units)")
    st.session_state["bk_start_bankroll"] = st.number_input(
        "Starting bankroll", min_value=0.0, value=float(st.session_state["bk_start_bankroll"]),
        step=10.0, label_visibility="collapsed"
    )
    st.checkbox("Use parlay stake first", value=st.session_state["bk_use_parlay_first"],
                key="bk_use_parlay_first")

with colD:
    st.caption("Newest first")
    st.toggle("Newest first", value=st.session_state["bk_newest_first"], key="bk_newest_first",
              label_visibility="collapsed")
    run_bt = st.button("Run Backtest", use_container_width=True)

if st.button(f"All {MIN_SEASON}â€“{max_season}"):
    try:
        st.session_state["bk_weeks"] = wk_opts
        st.rerun()
    except Exception:
        st.rerun()

# Filter
base = edges.copy()
if selected_seasons:
    base = base[base["season"].isin(selected_seasons)]
if st.session_state["bk_weeks"]:
    base = base[base["week"].isin(st.session_state["bk_weeks"])]

# Enforce 2017+
if "season" in base.columns:
    before, base = len(base), base[pd.to_numeric(base["season"], errors="coerce").fillna(0) >= MIN_SEASON]
    after = len(base)
    if before != after:
        st.caption(f"Backtest trimmed to **{MIN_SEASON}+** (rows: {after:,}/{before:,}).")

# Collapse to parlays if requested (ensure grouping BEFORE collapsing)
is_parlay_ui = bool(st.session_state.get("bk_mode_parlay", False))
if is_parlay_ui:
    base_grouped, group_col, _ = ensure_parlay_grouping(base, parlay_mode=True)
    work = legs_to_parlays(base_grouped)
    is_parlay_mode = not work.empty
    if not is_parlay_mode:
        st.warning("Parlay mode ON but no grouping fields found in the data; using singles for this run.")
else:
    work = base.copy()
    is_parlay_mode = False

# Guarantee sort_ts exists
work = ensure_sort_ts(work)
# ---- Rows debug banner ----
try:
    _dbg = locals().get("_dbg", {})
    if not _dbg:
        _dbg = {}
        _dbg["raw"] = len(edges)
        _dbg["after_season"]  = len(base)
        _dbg["after_week"]    = len(base)
        _dbg["after_settled"] = len(base)
    _dbg["final_work"] = 0 if work is None else len(work)
    st.caption(
        f"Rows: raw -> after season -> after week -> after settled -> final work  "
        f"{_dbg['raw']:,} -> {_dbg['after_season']:,} -> {_dbg['after_week']:,} -> {_dbg['after_settled']:,} -> {_dbg['final_work']:,}"
    )
except Exception:
    pass
# ---------------------------




# Require explicit run
if not run_bt:
    st.info("Adjust filters, then press **Run Backtest**.")
    st.stop()

# Summary metrics
stats = summarize(work, use_parlay_first=st.session_state["bk_use_parlay_first"])

m1, m2, m3, m4, m5, m6 = st.columns(6)
with m1:
    st.metric("Total Bets", f"{stats['total_bets']:,}")
with m2:
    st.metric("Wins / Losses / Pushes", f"{stats['wins']:,} / {stats['losses']:,} / {stats['pushes']:,}")
with m3:
    st.metric("Win %", f"{(stats['win_pct']*100):.1f}%")
with m4:
    st.metric("Avg EV", "-" if np.isnan(stats["avg_ev"]) else f"{stats['avg_ev']:.3f}")
with m5:
    st.metric("Stake Î£ (units)", f"{stats['total_stake']:.2f}")
with m6:
    st.metric("Profit (units)", f"{stats['total_profit']:.2f}")

m7, m8, m9 = st.columns(3)
with m7:
    st.metric("ROI", f"{(stats['roi']*100):.2f}%")
with m8:
    st.metric("Max Drawdown (units)", f"{stats['max_drawdown']:.2f}")
with m9:
    st.metric("Final Equity (units)", f"{stats['final_equity'] + float(st.session_state['bk_start_bankroll']):.2f}")

# Equity curve
st.subheader("Equity curve")
eq = equity_curve(work, starting_bankroll=float(st.session_state["bk_start_bankroll"]),
                  use_parlay_first=st.session_state["bk_use_parlay_first"])
if eq.empty:
    st.caption("No equity points to plot.")
else:
    st.line_chart(eq.set_index("ts")["equity"])

# Near-miss parlays (leg level required; show from base legs df)
st.markdown("### Near-miss parlays (exactly one leg lost)")
show_near_table = st.checkbox("Show near-miss table", value=True, key="bk_show_near_table")
near_df = build_near_misses(base)  # use leg-level base
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Near-miss count", int(near_df.shape[0]) if not near_df.empty else 0)
with col2:
    total_parlays = 0
    key_col = next((k for k in ("parlay_id","slip_id","ticket_id","bet_id") if k in base.columns), None)
    if key_col:
        total_parlays = int(base.groupby(key_col).ngroups)
    st.metric("Total parlays", total_parlays)
with col3:
    rate = (near_df.shape[0] / total_parlays * 100.0) if total_parlays else 0.0
    st.metric("Near-miss rate", f"{rate:.2f}%")
if show_near_table and not near_df.empty:
    st.dataframe(near_df, use_container_width=True, height=340)
    st.download_button("Download near_misses.csv", near_df.to_csv(index=False).encode("utf-8"),
                       "near_misses.csv", "text/csv")
else:
    st.caption("No near-miss parlays found (or parlay grouping/result columns missing).")

# Edges preview + download
st.subheader("Edges preview")
st.dataframe(work, use_container_width=True, hide_index=True, height=420)
csv = work.to_csv(index=False).encode("utf-8-sig")
st.download_button("Download filtered picks.csv", data=csv, file_name="filtered_picks.csv", mime="text/csv")



