from __future__ import annotations
# -*- coding: utf-8 -*-
# Backtest — seasons/weeks filters, full analytics, equity curve, edges preview (wide + clean)



import os
from pathlib import Path

import pandas as pd
import streamlit as st
# add these
import numpy as np
from typing import Iterable, Tuple
from datetime import datetime
st.set_page_config(page_title="Backtest", layout="wide")

# ===================
# Paths & IO helpers
# ===================
def _find_edges_csv() -> Path:
    # 1) allow override via env var
    env = os.environ.get("EDGES_CSV")
    if env:
        p = Path(env)
        if p.exists():
            return p

    # 2) walk upward from this file until we find exports/edges.csv
    here = Path(__file__).resolve()
    for base in [here.parent, *here.parents]:
        candidate = base / "exports" / "edges.csv"
        if candidate.exists():
            return candidate

    # 3) last-ditch guess: repo-root/exports/edges.csv
    # (adjust parents depth if your layout differs)
    return here.parents[3] / "exports" / "edges.csv"

EDGES_CSV = _find_edges_csv()
st.caption(f"Reading edges from: `{EDGES_CSV}`")

def read_csv_safe(path: Path) -> pd.DataFrame:
    if not path.exists() or path.stat().st_size == 0:
        return pd.DataFrame()
    for kw in ({"encoding": "utf-8-sig"}, {}, {"engine": "python"}):
        try:
            return pd.read_csv(path, **kw)
        except Exception:
            continue
    return pd.DataFrame()



# ============
# Normalizers
# ============
def am_to_dec(american) -> float | None:
    try:
        a = float(american)
    except Exception:
        return None
    if a == 0:
        return None
    return 1.0 + (a / 100.0 if a > 0 else 100.0 / abs(a))

def load_edges() -> pd.DataFrame:
    df = read_csv_safe(EDGES_CSV)
    if df.empty:
        return df

    df = df.copy()
    # normalize columns
    df.columns = [str(c).strip().lower() for c in df.columns]
    need = [
        "ts", "sort_ts", "sport", "league", "season", "week", "market", "ref",
        "side", "line", "odds", "p_win", "ev", "result", "stake", "parlay_stake",
        "game_id"
    ]
    for c in need:
        if c not in df.columns:
            df[c] = None

    # types
    df["sort_ts"] = pd.to_datetime(df["ts"] if df["sort_ts"].isna().all() else df["sort_ts"], errors="coerce")
    df["season"] = pd.to_numeric(df["season"], errors="coerce").astype("Int64")
    df["week"] = pd.to_numeric(df["week"], errors="coerce").astype("Int64")
    df["odds"] = pd.to_numeric(df["odds"], errors="coerce")
    df["p_win"] = pd.to_numeric(df["p_win"], errors="coerce")
    df["ev"] = pd.to_numeric(df["ev"], errors="coerce")
    df["stake"] = pd.to_numeric(df["stake"], errors="coerce")
    df["parlay_stake"] = pd.to_numeric(df["parlay_stake"], errors="coerce")
    df["result"] = df["result"].astype(str).str.lower().where(df["result"].notna(), None)

    return df

def discover_seasons(df: pd.DataFrame) -> list[int]:
    vals = sorted({int(x) for x in df["season"].dropna().unique().tolist()})
    return vals

def compute_row_profit(row: pd.Series, use_parlay_first: bool = True) -> Tuple[float, float]:
    """
    Returns (stake_used, profit_units) for a single row.
    - stake_used: stake actually considered (parlay_stake > stake > 1.0 fallback)
    - profit_units: +win profit, -lose stake, 0 for push/unknown
    """
    # which stake column to take
    stake = None
    if use_parlay_first and not pd.isna(row.get("parlay_stake")):
        stake = float(row.get("parlay_stake"))
    elif not pd.isna(row.get("stake")):
        stake = float(row.get("stake"))
    else:
        stake = 1.0

    if stake is None or stake <= 0:
        stake = 1.0

    # odds conversion
    dec = am_to_dec(row.get("odds"))
    result = str(row.get("result") or "").lower()

    if dec is None or pd.isna(dec):
        return stake, 0.0

    if result == "win":
        return stake, stake * (dec - 1.0)
    if result == "lose":
        return stake, -stake
    if result == "push":
        return stake, 0.0

    # ungraded: do not affect PnL
    return stake, 0.0
def _parlay_key(df: pd.DataFrame) -> str | None:
    for k in ("parlay_id", "slip_id", "ticket_id", "bet_id"):
        if k in df.columns:
            return k
    return None

def _combined_decimal_odds(group: pd.DataFrame) -> float | None:
    decs = []
    for _, r in group.iterrows():
        d = am_to_dec(r.get("odds"))
        if d is None or pd.isna(d):
            return None
        decs.append(float(d))
    if not decs:
        return None
    prod = 1.0
    for d in decs:
        prod *= d
    return prod

def _parlay_result(group: pd.DataFrame) -> str:
    # if any lose -> lose; else if any unknown -> unknown; else if any push -> push; else win
    res = group["result"].astype(str).str.lower().fillna("")
    if (res == "lose").any():
        return "lose"
    if (res == "win").all():
        return "win"
    # handle push / voids (no legs lost): treat as push for parlay
    if (res == "push").any():
        return "push"
    return ""  # unknown/ungraded -> ignore PnL

def legs_to_parlays(df: pd.DataFrame) -> pd.DataFrame:
    """Collapse leg rows into one parlay row per parlay_id with combined odds and single stake."""
    df = df.copy()
    key = _parlay_key(df)
    if key is None:
        return pd.DataFrame()  # no parlay grouping present

    groups = []
    for pid, g in df.groupby(key, dropna=False):
        # choose a stake for the parlay: prefer parlay_stake, else stake, else 1.0
        pstake = pd.to_numeric(g.get("parlay_stake"), errors="coerce").dropna()
        stake = float(pstake.iloc[0]) if not pstake.empty else float(pd.to_numeric(g.get("stake"), errors="coerce").dropna().iloc[0] if pd.to_numeric(g.get("stake"), errors="coerce").notna().any() else 1.0)

        dec = _combined_decimal_odds(g)
        result = _parlay_result(g)

        # pick something representative for season/week/time
        sort_ts = pd.to_datetime(g["sort_ts"], errors="coerce").min()
        season  = pd.to_numeric(g["season"], errors="coerce").dropna().astype(int)
        week    = pd.to_numeric(g["week"], errors="coerce").dropna().astype(int)
        season  = int(season.iloc[0]) if not season.empty else None
        week    = int(week.iloc[0]) if not week.empty else None

        # store a single row for this parlay
        groups.append(dict(
            sort_ts=sort_ts,
            season=season,
            week=week,
            market="PARLAY",
            ref=str(pid),
            odds=None,            # we keep leg odds separately; dec_comb below
            dec_comb=dec,         # combined decimal odds (product of legs)
            parlay_stake=stake,
            result=result,
            legs=len(g),
        ))

    out = pd.DataFrame(groups)
    # align with the columns our page expects
    if "stake" not in out.columns:
        out["stake"] = None
    if "p_win" not in out.columns:
        out["p_win"] = None
    if "ev" not in out.columns:
        out["ev"] = None
    out["ts"] = out["sort_ts"]
    return out

def compute_parlay_row_profit(row: pd.Series) -> tuple[float, float]:
    """PnL for a single parlay row returned by legs_to_parlays()."""
    stake = float(row.get("parlay_stake") or 1.0)
    dec   = row.get("dec_comb")
    res   = str(row.get("result") or "").lower()
    if dec is None or pd.isna(dec):
        return stake, 0.0
    if res == "win":
        return stake, stake * (float(dec) - 1.0)
    if res == "lose":
        return stake, -stake
    if res == "push":
        return stake, 0.0
    return stake, 0.0

# ==================
# Analytics helpers
# ==================
# --- Near-miss section (Backtest) --------------------------------------------
import pandas as pd
st.markdown("### Near-miss parlays (exactly one leg lost)")
show_near_table = st.checkbox("Show near-miss table", value=True, key="bk_show_near_table")

near_df = pd.DataFrame()
try:
    # If your Backtest builds an intermediate `parlays_df`, pass that here.
    # Otherwise, we try the same `work`/`bets_df` if they already include parlay rows.
    source_df = None
    for candidate in ("parlays_df", "work", "bets_df"):
        if candidate in globals() and isinstance(globals()[candidate], pd.DataFrame):
            source_df = globals()[candidate]
            break
    if source_df is not None and not source_df.empty:
        near_df = build_near_misses(source_df)

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Near-miss count", int(near_df.shape[0]) if not near_df.empty else 0)
    with col2:
        total_parlays = 0
        key_col, res_col = _pick_parlay_key_cols(source_df) if source_df is not None else (None, None)
        if key_col:
            total_parlays = int(source_df.groupby(key_col).ngroups)
        st.metric("Total parlays", total_parlays)
    with col3:
        rate = (near_df.shape[0] / total_parlays * 100.0) if total_parlays else 0.0
        st.metric("Near-miss rate", f"{rate:.2f}%")

    if show_near_table and not near_df.empty:
        st.dataframe(near_df, use_container_width=True, height=340)

    if not near_df.empty:
        csv = near_df.to_csv(index=False).encode("utf-8")
        st.download_button("Download near_misses.csv", csv, "near_misses.csv", "text/csv")
    else:
        st.caption("No near-miss parlays found (or parlay grouping/result columns missing).")
except Exception as _e:
    st.caption("Near-miss view unavailable (data not grouped by parlay).")

def equity_curve(work: pd.DataFrame, starting_bankroll: float = 100.0, use_parlay_first: bool = True) -> pd.DataFrame:
    if work.empty:
        return pd.DataFrame({"ts": [], "equity": []})
    W = work.sort_values("sort_ts", ascending=True, na_position="last").copy()
    running = float(starting_bankroll)
    eq = []
    for _, r in W.iterrows():
        stake, pnl = compute_row_profit(r, use_parlay_first=use_parlay_first)
        running += pnl
        eq.append(running)
    return pd.DataFrame({"ts": W["sort_ts"].values, "equity": eq})

def max_drawdown(equity_series: Iterable[float]) -> Tuple[float, float, float]:
    """
    Returns (max_drawdown, peak, trough) in units.
    """
    max_dd = 0.0
    peak = -np.inf
    trough = -np.inf
    run_peak = -np.inf
    for v in equity_series:
        run_peak = max(run_peak, v)
        dd = run_peak - v
        if dd > max_dd:
            max_dd = dd
            peak = run_peak
            trough = v
    return float(max_dd), float(peak if peak != -np.inf else 0.0), float(trough if trough != -np.inf else 0.0)

def summarize(work: pd.DataFrame, use_parlay_first: bool = True, is_parlay_mode: bool = False) -> dict:
    total_bets = len(work)
    wins = (work["result"] == "win").sum()
    losses = (work["result"] == "lose").sum()
    pushes = (work["result"] == "push").sum()

    stakes = []
    profits = []
    for _, r in work.iterrows():
        if is_parlay_mode:
            s, p = compute_parlay_row_profit(r)
        else:
            s, p = compute_row_profit(r, use_parlay_first=use_parlay_first)
        stakes.append(float(s))
        profits.append(float(p))


    win_pct = (wins / total_bets) if total_bets else 0.0
    roi = (total_profit / total_stake) if total_stake > 0 else 0.0

    avg_ev = float(pd.to_numeric(work.get("ev"), errors="coerce").dropna().mean()) if "ev" in work.columns else np.nan

    # Equity metrics (units)
    eq = equity_curve(work, starting_bankroll=0.0, use_parlay_first=use_parlay_first)
    max_dd, peak_eq, trough_eq = (0.0, 0.0, 0.0)
    final_eq = 0.0
    if not eq.empty:
        max_dd, peak_eq, trough_eq = max_drawdown(eq["equity"].tolist())
        final_eq = float(eq["equity"].iloc[-1])

    return dict(
        total_bets=total_bets,
        wins=int(wins),
        losses=int(losses),
        pushes=int(pushes),
        win_pct=win_pct,
        avg_ev=avg_ev,
        total_stake=total_stake,
        total_profit=total_profit,
        roi=roi,
        max_drawdown=max_dd,
        peak_equity=peak_eq,
        final_equity=final_eq,
    )

# ==========
# UI layout
# ==========
st.title("Backtest")

edges = load_edges()
if edges.empty:
    st.info("No edges found in exports/edges.csv")
    st.stop()
with st.expander("Data source & seasons (debug)"):
    st.write("EDGES_CSV:", str(EDGES_CSV))
    if not edges.empty:
        seasons_seen = sorted(pd.to_numeric(edges["season"], errors="coerce").dropna().astype(int).unique().tolist())
        st.write(f"Seasons detected ({len(seasons_seen)}):", seasons_seen[:25], "…" if len(seasons_seen) > 25 else "")
    else:
        st.warning("No rows loaded.")

# Discover seasons dynamically (e.g., 1966–2025)
all_seasons = discover_seasons(edges)
if not all_seasons:
    all_seasons = [datetime.now().year]

# Default state
st.session_state.setdefault("bk_start_bankroll", 100.0)
st.session_state.setdefault("bk_newest_first", True)
st.session_state.setdefault("bk_weeks", [])
st.session_state.setdefault("bk_use_parlay_first", True)

# Top control row
colA, colB, colC, colD = st.columns([2.2, 2.8, 1.2, 1.6])

with colA:
    st.caption("Seasons")
    season_labels = ["All"] + [str(s) for s in all_seasons]
    default_sel = ["All"]
    selected = st.multiselect("Select seasons", season_labels, default=default_sel, label_visibility="collapsed")
    if "All" in selected or not selected:
        selected_seasons = all_seasons
    else:
        selected_seasons = [int(s) for s in selected]

with colB:
    st.caption("Weeks (1–22)")
    w_default = st.session_state.get("bk_weeks", [])
    weeks_pick = st.multiselect("Select weeks", list(range(1, 23)), default=w_default, label_visibility="collapsed")
    c1, c2 = st.columns(2)
    with c1:
        if st.button("All weeks", use_container_width=True):
            st.session_state["bk_weeks"] = list(range(1, 23))
            weeks_pick = st.session_state["bk_weeks"]
    with c2:
        if st.button("Clear weeks", use_container_width=True):
            st.session_state["bk_weeks"] = []
            weeks_pick = []
    st.session_state["bk_weeks"] = weeks_pick

with colC:
    st.caption("Starting bankroll (units)")
    st.session_state["bk_start_bankroll"] = st.number_input(
        "Starting bankroll", min_value=0.0, value=float(st.session_state["bk_start_bankroll"]),
        step=10.0, label_visibility="collapsed"
    )
    st.checkbox("Use parlay stake first", value=st.session_state["bk_use_parlay_first"],
                key="bk_use_parlay_first", help="Prefer parlay_stake over stake when present.")

with colD:
    st.caption("Newest first")
    st.toggle("Newest first", value=st.session_state["bk_newest_first"], key="bk_newest_first", label_visibility="collapsed")
    run_bt = st.button("Run Backtest", use_container_width=True)

with st.expander("Diagnostics / ETL (optional)"):
    st.caption("Wire a CLI ETL or debugging widgets here if needed.")
st.session_state.setdefault("bk_mode_parlay", False)
st.toggle("Parlay mode", value=st.session_state["bk_mode_parlay"], key="bk_mode_parlay")

# Apply filters
base = edges.copy()

# Filter by seasons/weeks first
if selected_seasons:
    base = base[base["season"].isin(selected_seasons)]
if weeks_pick:
    base = base[base["week"].isin(weeks_pick)]

if st.session_state["bk_mode_parlay"]:
    # Collapse to parlays if we have a grouping id
    P = legs_to_parlays(base)
    if P.empty:
        st.warning("Parlay mode is on but no grouping column (parlay_id/slip_id/ticket_id) was found. Showing empty set.")
    work = P
else:
    work = base

# Sort order for preview/equity
work = work.sort_values("sort_ts", ascending=not st.session_state["bk_newest_first"], na_position="last")


# Summary metrics
stats = summarize(work, use_parlay_first=st.session_state["bk_use_parlay_first"],
                  is_parlay_mode=st.session_state["bk_mode_parlay"])


m1, m2, m3, m4, m5, m6 = st.columns(6)
with m1:
    st.metric("Total Bets", f"{stats['total_bets']:,}")
with m2:
    st.metric("Wins / Losses / Pushes", f"{stats['wins']:,} / {stats['losses']:,} / {stats['pushes']:,}")
with m3:
    st.metric("Win %", f"{(stats['win_pct']*100):.1f}%")
with m4:
    st.metric("Avg EV", "-" if np.isnan(stats["avg_ev"]) else f"{stats['avg_ev']:.3f}")
with m5:
    st.metric("Stake Σ (units)", f"{stats['total_stake']:.2f}")
with m6:
    st.metric("Profit (units)", f"{stats['total_profit']:.2f}")

m7, m8, m9 = st.columns(3)
with m7:
    st.metric("ROI", f"{(stats['roi']*100):.2f}%")
with m8:
    st.metric("Max Drawdown (units)", f"{stats['max_drawdown']:.2f}")
with m9:
    st.metric("Final Equity (units)", f"{stats['final_equity'] + float(st.session_state['bk_start_bankroll']):.2f}")

# Equity curve (units)
st.subheader("Equity curve")
eq = (work.sort_values("sort_ts").assign(
        __pnl = [
            (compute_parlay_row_profit(r)[1] if st.session_state["bk_mode_parlay"]
             else compute_row_profit(r, use_parlay_first=st.session_state["bk_use_parlay_first"])[1])
            for _, r in work.iterrows()
        ]
     ))
running = float(st.session_state["bk_start_bankroll"])
equity = []
for v in eq["__pnl"]:
    running += float(v)
    equity.append(running)
eq = pd.DataFrame({"ts": work.sort_values("sort_ts")["sort_ts"].values, "equity": equity})


# Edges preview + download
st.subheader("Edges preview")
st.dataframe(work, use_container_width=True, hide_index=True, height=420)

csv = work.to_csv(index=False).encode("utf-8-sig")
st.download_button("Download filtered picks.csv", data=csv, file_name="filtered_picks.csv", mime="text/csv")
