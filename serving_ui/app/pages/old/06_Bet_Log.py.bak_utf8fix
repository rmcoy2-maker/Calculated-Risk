from pathlib import Path
import pandas as pd
import numpy as np
import streamlit as st
import sys
from typing import List, Dict, Any, Tuple, Optional, Iterable
import re
import json
import ast
import io
from datetime import date, datetime, timedelta
try:
    import requests
except Exception:
    requests = None  # type: ignore


# Allow absolute imports inside Streamlit
try:
    from app.bootstrap import bootstrap_paths
    bootstrap_paths()
except Exception:
    pass
    from app.bootstrap import bootstrap_paths  # type: ignore
    bootstrap_paths()
except Exception:
    # Fallback no-op if the project bootstrap is not importable
    def bootstrap_paths() -> None:  # type: ignore
        return None

# ---- PATH SHIM (make 'serving_ui' parent importable) ----
_APP_DIR = Path(__file__).resolve().parents[1]  # ...\serving_ui\app
_PARENT  = _APP_DIR.parent                      # ...\serving_ui
if str(_PARENT) not in sys.path:
    sys.path.insert(0, str(_PARENT))

# ---- PROJECT ROOT (make top-level 'app' importable if needed) ----
PROJ_ROOT = Path(__file__).resolve().parents[3]
if str(PROJ_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJ_ROOT))

# --------------------------------------------------------------------------------------------------------------
# Page config
# --------------------------------------------------------------------------------------------------------------
st.set_page_config(page_title="Bet Log", page_icon="ÃƒÆ’Ã‚Â°Ãƒâ€¦Ã‚Â¸Ãƒâ€šÃ‚ÂÃƒâ€¹Ã¢â‚¬Â ", layout="wide")
st.title("Bet Log")
st.caption("Robust loader, filters, parlay settlement from legs, diagnostics, fuzzy matching, and summaries.")

# --------------------------------------------------------------------------------------------------------------
# Paths / Discovery
# --------------------------------------------------------------------------------------------------------------
def _discover_exports() -> Path:
    here = Path(__file__).resolve()
    candidates: Iterable[Path] = [
        here.parents[3] / "exports",
        here.parents[2] / "exports",
        here.parents[1] / "exports",
        here.parent / "exports",
    ]
    for p in candidates:
        try:
            if p.exists() and p.is_dir():
                return p
        except Exception:
            continue
    fallback = here.parent / "exports"
    fallback.mkdir(parents=True, exist_ok=True)
    return fallback

EXPORTS: Path = _discover_exports()
PARLAYS: Path = EXPORTS / "parlays.csv"
BETS_LOG: Path = EXPORTS / "bets_log.csv"
RESULTS: Path = EXPORTS / "results.csv"  # auto-created/updated

st.sidebar.subheader("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Data locations")
st.sidebar.code(str(EXPORTS))

# --------------------------------------------------------------------------------------------------------------
# Columns / Schemas
# --------------------------------------------------------------------------------------------------------------
PARLAY_COLS = [
    "ts","sport","league","game_id","market","ref","side","line","odds",
    "stake","p_win","ev","parlay_id","leg_index","payout","result","notes","placed_at"
]
BETLOG_COLS = [
    "ts","game_id","market","ref","side","line","odds","p_win","ev",
    "stake","placed_at","result","payout","notes"
]

# --------------------------------------------------------------------------------------------------------------
# IO helpers (defensive)
# --------------------------------------------------------------------------------------------------------------
def _ensure_headers(path: Path, header_line: str) -> None:
    try:
        if (not path.exists()) or path.stat().st_size == 0:
            path.parent.mkdir(parents=True, exist_ok=True)
            path.write_text(header_line + "\n", encoding="utf-8")
    except Exception as e:
        st.warning(f"Could not ensure headers for {path.name}: {e}")

def _normalize_columns(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    out = df.copy()
    for c in cols:
        if c not in out.columns:
            out[c] = pd.NA
    return out[cols].copy()

def _coerce_datetime(df: pd.DataFrame, colnames: Iterable[str]) -> pd.DataFrame:
    out = df.copy()
    for c in colnames:
        if c in out.columns:
            out[c] = pd.to_datetime(out[c], errors="coerce", utc=True)
    return out

@st.cache_data(ttl=20, show_spinner=False)
def _read_csv_safe(path: Path, cols: List[str]) -> pd.DataFrame:
    try:
        if not path.exists() or path.stat().st_size == 0:
            return pd.DataFrame(columns=cols)
        try:
            df = pd.read_csv(path, encoding="utf-8-sig")
        except pd.errors.EmptyDataError:
            return pd.DataFrame(columns=cols)
        if df is None or df.empty:
            return pd.DataFrame(columns=cols)
        df = _normalize_columns(df, cols)
        return df
    except Exception as e:
        st.warning(f"Could not read {path.name}: {e}")
        return pd.DataFrame(columns=cols)

# --------------------------------------------------------------------------------------------------------------
# Load data
# --------------------------------------------------------------------------------------------------------------
_ensure_headers(PARLAYS, ",".join(PARLAY_COLS))
_ensure_headers(BETS_LOG, ",".join(BETLOG_COLS))
_ensure_headers(RESULTS, "game_id,home_score,away_score,home,away,total_points")

parlays_df = _read_csv_safe(PARLAYS, PARLAY_COLS)
bets_df    = _read_csv_safe(BETS_LOG, BETLOG_COLS)

parlays_df = _coerce_datetime(parlays_df, ["ts","placed_at"])
bets_df    = _coerce_datetime(bets_df, ["ts","placed_at"])

if not parlays_df.empty:
    parlays_df = parlays_df.assign(origin="suggested")
if not bets_df.empty:
    bets_df = bets_df.assign(origin="placed")

# --------------------------------------------------------------------------------------------------------------
# Results loader (for settling legs) with fuzzy keys + mtime cache
# --------------------------------------------------------------------------------------------------------------
def _results_mtime() -> float:
    try:
        return RESULTS.stat().st_mtime
    except FileNotFoundError:
        return 0.0

def _norm_gid(gid: str) -> Tuple[str,str]:
    gid = str(gid or "")
    exact = gid.upper().strip()
    m = re.search(r"([A-Z]{2,4}@[A-Z]{2,4})$", exact)
    fuzzy = m.group(1) if m else exact
    return exact, fuzzy

@st.cache_data(ttl=0, show_spinner=False)
def load_results_map(_mtime: float) -> Dict[str, Dict[str, Any]]:
    try:
        df = pd.read_csv(RESULTS, encoding="utf-8-sig")
    except Exception:
        return {}
    cols = {c.lower(): c for c in df.columns}
    gid = cols.get("game_id") or cols.get("gid") or cols.get("gameid")
    hs  = cols.get("home_score") or cols.get("home_pts") or cols.get("home_points") or cols.get("score_home")
    as_ = cols.get("away_score") or cols.get("away_pts") or cols.get("away_points") or cols.get("score_away")
    home = cols.get("home") or cols.get("home_team") or cols.get("team_home") or cols.get("home_abbrev")
    away = cols.get("away") or cols.get("away_team") or cols.get("team_away") or cols.get("away_abbrev")
    total = cols.get("total_points")
    if not gid or not hs or not as_:
        return {}
    keep = [gid, hs, as_] + [c for c in [home, away, total] if c]
    df = df[keep].copy()
    df.rename(columns={gid:"game_id", hs:"home_score", as_:"away_score"}, inplace=True)
    if home: df.rename(columns={home:"home"}, inplace=True)
    if away: df.rename(columns={away:"away"}, inplace=True)
    if total: df.rename(columns={total:"total_points"}, inplace=True)
    if "total_points" not in df.columns:
        df["total_points"] = pd.to_numeric(df["home_score"], errors="coerce") + pd.to_numeric(df["away_score"], errors="coerce")
    out: Dict[str, Dict[str, Any]] = {}
    for _, r in df.iterrows():
        record = dict(r)
        exact, fuzzy = _norm_gid(record.get("game_id",""))
        out[exact] = record
        out[fuzzy] = record
        h = str(record.get("home","") or "").upper()
        a = str(record.get("away","") or "").upper()
        if h and a:
            out[f"{a}@{h}"] = record  # away@home
    return out

RESULTS_MAP: Dict[str, Dict[str, Any]] = load_results_map(_results_mtime())

# --------------------------------------------------------------------------------------------------------------
# Team normalization
# --------------------------------------------------------------------------------------------------------------
TEAM_MAP = {
    "NE":"NE","NEP":"NE","NWE":"NE",
    "GB":"GB","GBP":"GB",
    "KC":"KC","KAN":"KC",
    "PHI":"PHI","PHL":"PHI",
    "DAL":"DAL","CHI":"CHI","NYJ":"NYJ","BUF":"BUF",
}
def _norm_team(s: str) -> str:
    s = (s or "").upper()
    return TEAM_MAP.get(s, s)

# --------------------------------------------------------------------------------------------------------------
# Leg parsing & settlement
# --------------------------------------------------------------------------------------------------------------
def _safe_parse_legs(val: Any) -> List[Dict[str, Any]]:
    if isinstance(val, list):
        return val
    if not isinstance(val, str) or val.strip() == "":
        return []
    s = val.strip()
    try:
        obj = json.loads(s)
        if isinstance(obj, list):
            return obj
    except Exception:
        pass
    try:
        obj = ast.literal_eval(s)
        if isinstance(obj, list):
            return obj
    except Exception:
        return []
    return []

def _lookup_result_for_gid(gid: str) -> Optional[Dict[str, Any]]:
    exact, fuzzy = _norm_gid(gid)
    return RESULTS_MAP.get(exact) or RESULTS_MAP.get(fuzzy)

def _settle_leg(leg: Dict[str, Any], results: Dict[str, Dict[str, Any]]) -> str:
    gid = str(leg.get("game_id") or leg.get("gid") or "")
    r = _lookup_result_for_gid(gid) if gid else None
    if r is None:
        return "open"
    market = str(leg.get("market","")).upper()
    side = _norm_team(str(leg.get("side","")).upper())
    try:
        line = float(leg.get("line")) if leg.get("line") not in (None,"",pd.NA) else None
    except Exception:
        line = None
    home = _norm_team(str(r.get("home","")).upper())
    away = _norm_team(str(r.get("away","")).upper())
    hs = float(r.get("home_score", float("nan")))
    as_ = float(r.get("away_score", float("nan")))
    total_pts = float(r.get("total_points", hs+as_))

    if market in {"ML","MONEYLINE"}:
        if side in {home, away}:
            if hs > as_ and side == home: return "win"
            if as_ > hs and side == away: return "win"
            if hs == as_: return "push"
            return "loss"
        return "open"

    if market in {"ATS","SPREAD","HANDICAP"} and line is not None and side:
        margin = hs - as_ if side == home else as_ - hs if side == away else None
        if margin is None: return "open"
        adj_margin = margin + line
        if adj_margin > 0: return "win"
        if adj_margin == 0: return "push"
        return "loss"

    if market in {"TOTAL","OU","O/U","O-U"} and line is not None:
        if side in {"OVER","O"}:
            if total_pts > line: return "win"
            if total_pts == line: return "push"
            return "loss"
        if side in {"UNDER","U"}:
            if total_pts < line: return "win"
            if total_pts == line: return "push"
            return "loss"
        return "open"

    return "open"

def _settle_parlay_from_legs(row: pd.Series, results: Dict[str, Dict[str, Any]]) -> Optional[str]:
    if str(row.get("market","")).upper() != "PARLAY":
        return None
    legs = _safe_parse_legs(row.get("side") or row.get("ref"))
    if not legs: return None
    statuses = [_settle_leg(leg, results) for leg in legs]
    if any(s == "loss" for s in statuses): return "loss"
    if all(s == "push" for s in statuses): return "push"
    if all(s in {"win","push"} for s in statuses): return "win"
    return "open"

# --------------------------------------------------------------------------------------------------------------
# Unify rows
# --------------------------------------------------------------------------------------------------------------
def _unify_rows(parlays: pd.DataFrame, bets: pd.DataFrame) -> pd.DataFrame:
    def enrich(df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        if "placed_at" not in out.columns: out["placed_at"] = pd.NaT
        if "sport" not in out.columns:     out["sport"] = pd.NA
        if "league" not in out.columns:    out["league"] = pd.NA

        def american_to_decimal(o) -> float:
            try:
                o = float(o)
                if o >= 100:  return 1 + (o / 100.0)
                if o <= -100: return 1 + (100.0 / abs(o))
                return 1.0
            except Exception:
                return 1.0

        out["dec_odds"] = out.get("odds", pd.Series([], dtype="float64")).map(american_to_decimal)
        out["imp_prob"] = out["dec_odds"].apply(lambda d: 1.0 / d if d and d > 0 else pd.NA)
        return out

    p = enrich(parlays) if not parlays.empty else pd.DataFrame(columns=PARLAY_COLS + ["origin", "dec_odds", "imp_prob"])
    b = enrich(bets)    if not bets.empty    else pd.DataFrame(columns=BETLOG_COLS + ["origin", "dec_odds", "imp_prob"])

    display_cols = [
        "origin","ts","placed_at","sport","league","game_id","market","ref","side","line",
        "odds","dec_odds","imp_prob","p_win","ev","stake","result","payout","notes","parlay_id","leg_index"
    ]
    for c in display_cols:
        if c not in p.columns: p[c] = pd.NA
        if c not in b.columns: b[c] = pd.NA

    base = pd.concat([p[display_cols], b[display_cols]], ignore_index=True)
    base["sort_ts"] = base["placed_at"].fillna(base["ts"])
    base = base.sort_values("sort_ts", ascending=False, na_position="last").reset_index(drop=True)
    return base

base = _unify_rows(parlays_df, bets_df)

# --------------------------------------------------------------------------------------------------------------
# Outcome + PnL helpers
# --------------------------------------------------------------------------------------------------------------
def _label_outcome(row: pd.Series) -> str:
    r = str(row.get("result", "")).strip().lower()
    if r in {"win", "won"}: return "win"
    if r in {"loss", "lose", "lost"}: return "loss"
    if r in {"push", "void", "refund"}: return "push"

    settled = _settle_parlay_from_legs(row, RESULTS_MAP)
    if settled is not None and settled != "open": return settled

    stake = pd.to_numeric(row.get("stake", pd.NA), errors="coerce")
    payout = pd.to_numeric(row.get("payout", pd.NA), errors="coerce")
    if pd.notna(stake) and pd.notna(payout):
        if payout > stake:  return "win"
        if payout == stake: return "push"
        if payout == 0 and stake > 0: return "loss"

    return "open"

def _pnl_from_outcome(stake: float, dec_odds: float, outcome: str) -> float:
    if outcome == "win":  return stake * (dec_odds - 1.0)
    if outcome == "loss": return -stake
    if outcome == "push": return 0.0
    return float("nan")

def _attach_outcomes_and_pnl(df: pd.DataFrame, pretend_one_dollar: bool) -> pd.DataFrame:
    out = df.copy()
    # Default stake logic: blanks/zeros -> 1.0 unit (unless pretend toggle is on)
    out["stake_used"] = 1.0 if pretend_one_dollar else pd.to_numeric(
        out.get("stake", 1.0), errors="coerce"
    ).fillna(1.0).replace(0, 1.0)

    out["dec_odds"] = pd.to_numeric(out.get("dec_odds", 1.0), errors="coerce").fillna(1.0)
    out["outcome"]  = out.apply(_label_outcome, axis=1)

    out["pnl_realized"] = out.apply(
        lambda r: _pnl_from_outcome(float(r["stake_used"]), float(r["dec_odds"]), r["outcome"]),
        axis=1
    )

    pwin = pd.to_numeric(out.get("p_win", pd.NA), errors="coerce")
    out["pnl_expected"] = (pwin * (out["dec_odds"] - 1.0) - (1 - pwin)) * out["stake_used"]
    out.loc[pwin.isna(), "pnl_expected"] = pd.NA
    return out

# --------------------------------------------------------------------------------------------------------------
# Sidebar filters
# --------------------------------------------------------------------------------------------------------------
with st.sidebar:
    st.subheader("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â§ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â­ Filters")
    if RESULTS_MAP:
        st.success("Using exports/results.csv (fuzzy matching enabled).")
    else:
        st.info("Created an empty exports/results.csv for you. Use 'Paste scores' or 'Fetch NFL Results'.")

    pretend_toggle = st.checkbox(
        "Pretend $1 stake per row", value=False,
        help="Ignores the CSV stake column and uses $1 for performance math."
    )

    if not base.empty and base["sort_ts"].notna().any():
        max_dt = pd.to_datetime(base["sort_ts"]).max().to_pydatetime()
        min_dt = (max_dt - timedelta(days=14)).replace(hour=0, minute=0, second=0, microsecond=0)
        start_dt, end_dt = st.date_input(
            "Date range (by placed_at/ts)",
            value=(min_dt.date(), max_dt.date()),
            help="Filters by placed_at (or ts if placed_at is NA).",
        )
    else:
        st.date_input("Date range (by placed_at/ts)", disabled=True)
        start_dt, end_dt = None, None

    distinct_markets = sorted([m for m in base["market"].dropna().unique().tolist()]) if not base.empty else []
    market = st.selectbox("Market", options=["(all)"] + distinct_markets, index=0)

    origins = ["(all)"]
    if not parlays_df.empty: origins.append("suggested")
    if not bets_df.empty: origins.append("placed")
    origin_pick = st.selectbox("Origin", options=origins, index=0)

    leagues = sorted([m for m in base["league"].dropna().unique().tolist()]) if not base.empty else []
    league_pick = st.selectbox("League", options=["(all)"] + leagues, index=0)

def _apply_filters(df: pd.DataFrame,
                   start_dt: Optional[date],
                   end_dt: Optional[date],
                   market: str,
                   origin: str,
                   league: str) -> pd.DataFrame:
    out = df.copy()
    if start_dt and end_dt and out.shape[0] > 0:
        start_ts = pd.Timestamp(start_dt).tz_localize("UTC")
        end_ts = pd.Timestamp(end_dt + timedelta(days=1)).tz_localize("UTC")
        out = out[(out["sort_ts"].notna()) & (out["sort_ts"] >= start_ts) & (out["sort_ts"] < end_ts)]
    if market and market != "(all)":
        out = out[out["market"] == market]
    if origin and origin != "(all)":
        out = out[out["origin"] == origin]
    if league and league != "(all)":
        out = out[out["league"] == league]
    return out

filtered = _apply_filters(base, start_dt, end_dt, market, origin_pick, league_pick)
scored = _attach_outcomes_and_pnl(filtered, pretend_toggle)

# --------------------------------------------------------------------------------------------------------------
# Summary metrics
# --------------------------------------------------------------------------------------------------------------
col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
total_rows = int(scored.shape[0])
with col1: st.metric("Rows", f"{total_rows}")

with col2:
    st.metric("Total Stake Used", f"${scored['stake_used'].sum():,.2f}")

realized = scored.loc[scored["outcome"].isin(["win","loss","push"]), "pnl_realized"].fillna(0).sum()
with col3: st.metric("Realized PnL", f"${realized:,.2f}")

exp_pnl = pd.to_numeric(scored["pnl_expected"], errors="coerce").dropna().sum()
with col4: st.metric("Expected PnL (EV)", f"${exp_pnl:,.2f}")

placed_ct = int((scored["origin"] == "placed").sum())
sugg_ct   = int((scored["origin"] == "suggested").sum())
with col5: st.metric("Placed vs Suggested", f"{placed_ct} / {sugg_ct}")

placed_mask   = scored["origin"].eq("placed")
resolved_mask = scored["outcome"].isin(["win","loss","push"])
wins_placed   = int((placed_mask & scored["outcome"].eq("win")).sum())
bets_placed   = int((placed_mask & resolved_mask).sum())
win_pct       = (wins_placed / bets_placed) if bets_placed > 0 else 0.0
with col6: st.metric("Wins / Bets (Placed)", f"{wins_placed} / {bets_placed}")
with col7: st.metric("Win % (Placed)", f"{win_pct:.1%}")

# --------------------------------------------------------------------------------------------------------------
# Table
# --------------------------------------------------------------------------------------------------------------
st.subheader("Results")
if scored.empty:
    st.info("No rows to display. Add rows to exports/parlays.csv or exports/bets_log.csv.")
else:
    show_cols = [
        "origin","outcome","sort_ts","placed_at","ts","sport","league","game_id",
        "market","ref","side","line","odds","dec_odds","imp_prob","p_win","ev",
        "stake","stake_used","pnl_realized","pnl_expected","result","payout",
        "notes","parlay_id","leg_index"
    ]
    show_cols = [c for c in show_cols if c in scored.columns]
    view = scored[show_cols].copy().sort_values("sort_ts", ascending=False, na_position="last")

    for pc in ("imp_prob","p_win"):
        if pc in view.columns:
            view[pc] = pd.to_numeric(view[pc], errors="coerce").map(lambda v: f"{v:.1%}" if pd.notna(v) else "")

    for mc in ("stake","stake_used","ev","payout","pnl_realized","pnl_expected"):
        if mc in view.columns:
            view[mc] = pd.to_numeric(view[mc], errors="coerce").map(lambda v: f"${v:,.2f}" if pd.notna(v) else "")

    for tc in ("sort_ts","placed_at","ts"):
        if tc in view.columns:
            view[tc] = pd.to_datetime(view[tc], errors="coerce", utc=True).dt.strftime("%Y-%m-%d %H:%M:%S UTC")

    st.dataframe(view, use_container_width=True, hide_index=True)
    st.download_button(
        "ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¡ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¯ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Download filtered + scoring (CSV)",
        data=scored.to_csv(index=False).encode("utf-8"),
        file_name=f"bet_log_scored_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
    )

# --------------------------------------------------------------------------------------------------------------
# ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â§ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â© Paste scores (quick add/replace into exports/results.csv)
# --------------------------------------------------------------------------------------------------------------
with st.expander("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â§ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â© Paste scores ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¾ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ append to exports/results.csv", expanded=False):
    st.markdown(
        "Paste one game per line. Supported formats:\n"
        "- `2025-09-17-DAL@PHI 17-27`\n"
        "- `DAL@PHI 17-27`\n"
        "- `DAL@PHI 17 27`\n"
        "Optionally add team labels: `DAL@PHI 17-27, home=PHI, away=DAL`"
    )
    text = st.text_area("Lines", height=140, placehnewer="2025-09-17-DAL@PHI 17-27\nGB@CHI 20 24")
    if st.button("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Append/update results.csv"):
        try:
            cur = pd.read_csv(RESULTS, encoding="utf-8-sig")
        except Exception:
            cur = pd.DataFrame(columns=["game_id","home_score","away_score","home","away","total_points"])
        if cur.empty:
            cur = pd.DataFrame(columns=["game_id","home_score","away_score","home","away","total_points"])

        rows = []
        for line in text.splitlines():
            s = line.strip()
            if not s:
                continue
            m = re.match(r"(?i)^(?:(\d{4}-\d{2}-\d{2})-)?([A-Z]{2,4})@([A-Z]{2,4})\s+(\d+)[-\s](\d+)(?:.*home=([A-Z]{2,4}))?(?:.*away=([A-Z]{2,4}))?", s)
            if not m:
                continue
            date_part, away, home, hs, as_, home_lbl, away_lbl = m.groups()
            gid = f"{date_part}-{away}@{home}" if date_part else f"{away}@{home}"
            hs_i = int(hs); as_i = int(as_)
            rows.append({
                "game_id": gid,
                "home_score": hs_i,
                "away_score": as_i,
                "home": (home_lbl or home).upper(),
                "away": (away_lbl or away).upper(),
                "total_points": hs_i + as_i
            })

        if rows:
            new = pd.DataFrame(rows)
            def norm_gid(g: str) -> str: return (g or "").strip().upper()
            cur["_key"] = cur["game_id"].map(norm_gid)
            new["_key"] = new["game_id"].map(norm_gid)
            cur = cur[~cur["_key"].isin(new["_key"])].drop(columns=["_key"], errors="ignore")
            out = pd.concat([cur, new.drop(columns=["_key"], errors="ignore")], ignore_index=True)
            out.to_csv(RESULTS, index=False, encoding="utf-8")
            st.success(f"Added/updated {len(new)} games into {RESULTS.name}. Reloading...")
            st.rerun()
        else:
            st.warning("No valid lines parsed.")

# --------------------------------------------------------------------------------------------------------------
# Fetch NFL Results (nflverse) ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã¢â‚¬Â¦Ãƒâ€šÃ‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â built-in
# --------------------------------------------------------------------------------------------------------------
with st.expander(" Fetch NFL Results (nflverse)", expanded=False):
    st.caption("Pull completed NFL games from nflverse and upsert into exports/results.csv")
    colA, colB, colC = st.columns(3)
    default_seasons = [date.today().year - 1, date.today().year]
    seasons_text = colA.text_input("Seasons (space/comma sep)", value=" ".join(map(str, default_seasons)))
    since_text   = colB.text_input("Since (YYYY-MM-DD, optional)", value="")
    weeks_text   = colC.text_input("Weeks (space/comma sep, optional)", value="")

    def _parse_ints(s: str) -> List[int]:
        if not s.strip(): return []
        s = s.replace(",", " ")
        return [int(x) for x in s.split() if x.strip().isdigit()]

    def _fetch_schedule_csv(season: int) -> Optional[pd.DataFrame]:
        if requests is None:
            st.error("`requests` package not available in this environment.")
            return None
        URLS = [
            "https://raw.githubusercontent.com/nflverse/nflfastR-data/master/data/schedules/sched_{season}.csv",
            "https://raw.githubusercontent.com/nflverse/nflfastR-data/master/data/schedules/schedules_{season}.csv",
            "https://raw.githubusercontent.com/nflverse/nflverse-data/master/schedules/sched_{season}.csv",
            "https://raw.githubusercontent.com/nflverse/nflverse-data/master/schedules/schedules_{season}.csv",
        ]
        last_err = None
        for tmpl in URLS:
            url = tmpl.format(season=season)
            try:
                r = requests.get(url, timeout=30)
                if r.ok and r.text.strip():
                    return pd.read_csv(io.StringIO(r.text))
            except Exception as e:
                last_err = e
        if last_err:
            st.warning(f"Failed to fetch season {season}: {last_err}")
        else:
            st.warning(f"Failed to fetch season {season} from known URLs.")
        return None

    def _build_results_from_schedule(df: pd.DataFrame, weeks: Optional[List[int]], since: Optional[str]) -> pd.DataFrame:
        cols = {c.lower(): c for c in df.columns}
        week    = cols.get("week")
        gameday = cols.get("gameday") or cols.get("game_date") or cols.get("game_day") or cols.get("date")
        home    = cols.get("home_team") or cols.get("home")
        away    = cols.get("away_team") or cols.get("away")
        hs      = cols.get("home_score") or cols.get("score_home") or cols.get("home_points")
        as_     = cols.get("away_score") or cols.get("score_away") or cols.get("away_points")
        finished = cols.get("game_finished") or cols.get("result") or cols.get("status")
        if not all([gameday, home, away, hs, as_]):
            return pd.DataFrame(columns=["game_id","home_score","away_score","home","away","total_points"])

        out = df.copy()

        if weeks and week in out.columns:
            out = out[out[week].isin(list(weeks))]
        if finished and finished in out.columns:
            out = out[out[finished].astype(str).str.lower().isin(["true", "t", "1", "final", "post"])]
        out = out[pd.to_numeric(out[hs], errors="coerce").notna() & pd.to_numeric(out[as_], errors="coerce").notna()]

        if since:
            out_date = pd.to_datetime(out[gameday], errors="coerce").dt.date
            out = out[out_date >= pd.to_datetime(since).date()]

        out["game_date"] = pd.to_datetime(out[gameday], errors="coerce").dt.date
        out["home"] = out[home].astype(str).str.upper()
        out["away"] = out[away].astype(str).str.upper()
        out["home_score"] = pd.to_numeric(out[hs], errors="coerce").astype("Int64")
        out["away_score"] = pd.to_numeric(out[as_], errors="coerce").astype("Int64")
        out["game_id"] = out.apply(lambda r: f"{r['game_date']}-{r['away']}@r[r'home']" if False else f"{r['game_date']}-{r['away']}@{r['home']}", axis=1)
        out["total_points"] = out["home_score"].astype(float) + out["away_score"].astype(float)
        return out[["game_id", "home_score", "away_score", "home", "away", "total_points"]].dropna(subset=["home_score","away_score"])

    if st.button("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¡ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¯ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Fetch & Upsert"):
        seasons = _parse_ints(seasons_text) or default_seasons
        weeks   = _parse_ints(weeks_text) or None
        since   = since_text.strip() or None

        all_rows = []
        for s in seasons:
            df_sched = _fetch_schedule_csv(s)
            if df_sched is None:
                continue
            block = _build_results_from_schedule(df_sched, weeks=weeks, since=since)
            if not block.empty:
                all_rows.append(block)

        if all_rows:
            new = pd.concat(all_rows, ignore_index=True)
            try:
                cur = pd.read_csv(RESULTS, encoding="utf-8-sig")
            except Exception:
                cur = pd.DataFrame(columns=["game_id","home_score","away_score","home","away","total_points"])

            def norm_gid(g: str) -> str: return (g or "").strip().upper()
            cur["_key"] = cur["game_id"].map(norm_gid)
            new["_key"] = new["game_id"].map(norm_gid)

            cur = cur[~cur["_key"].isin(new["_key"])].drop(columns=["_key"], errors="ignore")
            out = pd.concat([cur, new.drop(columns=["_key"], errors="ignore")], ignore_index=True)
            out.to_csv(RESULTS, index=False, encoding="utf-8")
            st.success(f"Upserted {len(new)} games into {RESULTS.name}. Reloading to settle...")
            st.rerun()
        else:
            st.warning("No completed games fetched for the given filters.")

# --------------------------------------------------------------------------------------------------------------
# Settlement diagnostics (and skeleton writer)
# --------------------------------------------------------------------------------------------------------------
with st.expander("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚ÂºÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¯ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Settlement diagnostics", expanded=False):
    debug = scored.copy()
    open_parlays = debug[(debug["market"].str.upper() == "PARLAY") & (debug["outcome"] == "open")]
    st.write(f"Open parlays: {len(open_parlays)}")

    def explain(row: pd.Series) -> str:
        legs = _safe_parse_legs(row.get("side") or row.get("ref"))
        notes = []
        for i, leg in enumerate(legs, start=1):
            gid = leg.get("game_id") or leg.get("gid")
            verdict = _settle_leg(leg, RESULTS_MAP)
            mkt = str(leg.get("market",""))
            sd  = str(leg.get("side",""))
            ln  = str(leg.get("line",""))
            if verdict == "open":
                reason = []
                if not gid or _lookup_result_for_gid(str(gid)) is None:
                    reason.append("missing game_id in results")
                if mkt.upper() in {"ATS","SPREAD","HANDICAP","TOTAL","OU","O/U","O-U"} and (ln in (None,"",pd.NA)):
                    reason.append("missing line")
                notes.append(f"Leg {i}: {mkt} {sd} {ln} (gid={gid}) -> OPEN ({'; '.join(reason) or 'unknown'})")
            else:
                notes.append(f"Leg {i}: {mkt} {sd} {ln} (gid={gid}) -> {verdict.upper()}")
        return "\n".join(notes)

    if not open_parlays.empty:
        debug_view = open_parlays[["ts","ref","side","parlay_id","notes"]].copy()
        debug_view["diagnostics"] = open_parlays.apply(explain, axis=1)
        st.dataframe(debug_view, use_container_width=True, hide_index=True)

        def build_needed_results(df: pd.DataFrame) -> pd.DataFrame:
            rows = []
            for _, r in df.iterrows():
                legs = _safe_parse_legs(r.get("side") or r.get("ref"))
                for leg in legs:
                    gid = str(leg.get("game_id") or leg.get("gid") or "")
                    if gid and _lookup_result_for_gid(gid) is None:
                        _, fuzzy = _norm_gid(gid)
                        rows.append({"game_id": gid, "matchup": fuzzy, "home_score": "", "away_score": "", "home": "", "away": ""})
            if not rows:
                return pd.DataFrame(columns=["game_id","matchup","home_score","away_score","home","away"])
            out = pd.DataFrame(rows).drop_duplicates(subset=["game_id"])
            return out

        if st.button("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã‚Â¦ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Write needed_results.csv (skeleton) to exports/"):
            df_needed = build_needed_results(open_parlays)
            fp = EXPORTS / "needed_results.csv"
            df_needed.to_csv(fp, index=False, encoding="utf-8")
            st.success(f"Wrote {fp} with {len(df_needed)} rows to fill.")
    else:
        st.write("No open parlays to diagnose.")

# --------------------------------------------------------------------------------------------------------------
# Weekly ROI (closed only) & by origin ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã¢â‚¬Â¦Ãƒâ€šÃ‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â uses defaulted stake=1 for blanks/zeros
# --------------------------------------------------------------------------------------------------------------
def weekly_roi_closed(df_closed: pd.DataFrame) -> pd.DataFrame:
    out = df_closed.copy()
    out["ts"] = pd.to_datetime(out.get("sort_ts"), errors="coerce") \
    .combine_first(pd.to_datetime(out.get("ts"), errors="coerce"))
    out["stake_num"] = pd.to_numeric(out.get("stake"), errors="coerce").fillna(1.0).replace(0, 1.0)
    out["pnl_num"]   = pd.to_numeric(out.get("pnl_realized"), errors="coerce").fillna(0.0)
    wk = (
        out.set_index("ts")
           .resample("W")
           .agg(staked=("stake_num","sum"), pnl=("pnl_num","sum"))
    )
    with np.errstate(divide="ignore", invalid="ignore"):
        wk["ROI%"] = (wk["pnl"] / wk["staked"].replace(0, np.nan)) * 100
    return wk

def weekly_roi_by_origin(df_closed: pd.DataFrame) -> pd.DataFrame:
    out = df_closed.copy()
    out["ts"] = pd.to_datetime(out.get("sort_ts"), errors="coerce") \
    .combine_first(pd.to_datetime(out.get("ts"), errors="coerce"))
    out["stake_num"] = pd.to_numeric(out.get("stake"), errors="coerce").fillna(1.0).replace(0, 1.0)
    out["pnl_num"]   = pd.to_numeric(out.get("pnl_realized"), errors="coerce").fillna(0.0)
    wk = (
        out.groupby("origin")
           .apply(lambda g: g.set_index("ts").resample("W").agg(staked=("stake_num","sum"), pnl=("pnl_num","sum")))
           .reset_index()
    )
    with np.errstate(divide="ignore", invalid="ignore"):
        wk["ROI%"] = (wk["pnl"] / wk["staked"].replace(0, np.nan)) * 100
    return wk

with st.expander("Charts & breakdowns", expanded=False):
    try:
        import matplotlib.pyplot as plt  # avoid import error on some envs
        HAS_MPL = True
    except Exception:
        HAS_MPL = False

    if not HAS_MPL:
        st.info("Charts are disabled because matplotlib isn't installed. Install with: pip install matplotlib")
    else:
        # Weekly ROI% (closed only)
        closed_mask = scored["outcome"].isin(["win","loss","push"])
        df_closed = scored[closed_mask].copy()
        if not df_closed.empty:
            wk = weekly_roi_closed(df_closed)
            if not wk.empty and not wk["ROI%"].dropna().empty:
                fig = plt.figure()
                wk["ROI%"].plot()
                plt.ylabel("ROI %"); plt.title("Weekly ROI% (closed only)")
                st.pyplot(fig)

            # Weekly ROI% by origin
            if "origin" in df_closed.columns and df_closed["origin"].notna().any():
                wko = weekly_roi_by_origin(df_closed)
                if not wko.empty and not wko["ROI%"].dropna().empty:
                    fig, ax = plt.subplots()
                    for origin, group in wko.groupby("origin"):
                        ax.plot(group["ts"], group["ROI%"], marker="o", label=str(origin))
                    ax.set_ylabel("ROI %")
                    ax.set_title("Weekly ROI% by Origin")
                    ax.legend()
                    st.pyplot(fig)

# --------------------------------------------------------------------------------------------------------------
# Tables (normalize stake for display to show $1 default consistently)
# --------------------------------------------------------------------------------------------------------------
st.subheader("Open / Pending")
open_df = scored[scored["outcome"] == "open"].copy()
open_df["stake"] = pd.to_numeric(open_df.get("stake"), errors="coerce").fillna(1.0).replace(0, 1.0)
st.dataframe(
    open_df[["origin","bet_id","market","ref","side","odds","stake","notes"]]
    if "bet_id" in open_df.columns else
    open_df[["origin","market","ref","side","odds","stake","notes"]],
    use_container_width=True, hide_index=True
)

st.subheader("Closed / Settled")
closed_df = scored[scored["outcome"].isin(["win","loss","push"])].copy()
closed_df["stake"] = pd.to_numeric(closed_df.get("stake"), errors="coerce").fillna(1.0).replace(0, 1.0)
st.dataframe(
    closed_df[["origin","ts","market","ref","side","odds","stake","result","pnl_realized","payout","notes"]]
    if "payout" in closed_df.columns else
    closed_df[["origin","ts","market","ref","side","odds","stake","result","pnl_realized","notes"]],
    use_container_width=True, hide_index=True
)

st.subheader("All Entries (filtered)")
all_df = filtered.copy()
all_df["stake"] = pd.to_numeric(all_df.get("stake"), errors="coerce").fillna(1.0).replace(0, 1.0)
st.dataframe(
    all_df[["origin","ts","market","ref","side","odds","p_win","ev","stake","result","notes"]],
    use_container_width=True, hide_index=True
)

# --------------------------------------------------------------------------------------------------------------
# Export filtered
# --------------------------------------------------------------------------------------------------------------
csv_bytes = scored.to_csv(index=False, encoding="utf-8").encode("utf-8")
st.download_button(
    "ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¬ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¡ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¯ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â Download filtered bets (scored).csv",
    data=csv_bytes, file_name="filtered_bets_scored.csv", mime="text/csv"
)

# --------------------------------------------------------------------------------------------------------------
# Normalize & Save whole log (writes back to bets_log.csv with current columns)
# --------------------------------------------------------------------------------------------------------------
if st.button("ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã¢â‚¬Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â°ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€šÃ‚Â¦ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¸ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â§ÃƒÆ’Ã†â€™Ãƒâ€ Ã¢â‚¬â„¢ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã‚Â¡ÃƒÆ’Ã†â€™ÃƒÂ¢Ã¢â€šÂ¬Ã…Â¡ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â¹ Normalize & Save bets_log.csv"):
    # Recompose a saveable frame from base (before scoring) to avoid extra cols
    to_save_cols = [c for c in BETLOG_COLS if c in bets_df.columns]
    cleaned = bets_df[to_save_cols].copy() if not bets_df.empty else pd.DataFrame(columns=BETLOG_COLS)
    cleaned.to_csv(BETS_LOG, index=False, encoding="utf-8")
    st.success(f"Normalized and saved {BETS_LOG}")
    st.rerun()


# --- Parlay leg result sync (adds/updates "result" inside legs_json) ---
import json as _json
import pandas as _pd
import numpy as _np

def _cr_parse_legs_json(val):
    try:
        if _pd.isna(val): return []
        data = _json.loads(val) if isinstance(val, str) else val
        return data if isinstance(data, list) else []
    except Exception:
        return []

def _cr_leg_key(game_id, market, side, line):
    return (
        str(game_id or "").strip(),
        str(market or "").strip().lower(),
        str(side or "").strip(),
        str(line or "").strip(),
    )

def _cr_build_single_leg_result_index(df: _pd.DataFrame):
    """Map (game_id, market, side, line) -> result for settled single-leg bets."""
    if df is None or df.empty: return {}
    x = df.copy()
    x.columns = [c.lower().strip() for c in x.columns]
    # single-legs only, with a known result
    m = (x.get("game_id","").astype(str).str.upper() != "PARLAY")
    if "result" in x:
        m &= x["result"].astype(str).str.lower().isin(
            ["win","won","w","lose","lost","l","push","void","cancel","canceled","cancelled"]
        )
    else:
        return {}
    idx = {}
    for _, r in x.loc[m].iterrows():
        key = _cr_leg_key(r.get("game_id"), r.get("market"), r.get("side"), r.get("line"))
        idx[key] = str(r.get("result","")).lower()
    return idx

def sync_parlay_legs_into_json(bets_csv_path: str | Path) -> tuple[int,int]:
    """
    Loads bets_log.csv, writes 'result' into each parlay leg where we can match by (game_id,market,side,line).
    Returns (updated_parlays, updated_legs).
    """
    p = Path(bets_csv_path)
    try:
        df = _pd.read_csv(p)
    except Exception:
        return (0,0)
    df.columns = [c.lower().strip() for c in df.columns]
    if "game_id" not in df.columns: return (0,0)

    index = _cr_build_single_leg_result_index(df)
    if not index: return (0,0)

    is_parlay = df["game_id"].astype(str).str.upper().eq("PARLAY")
    upd_parlays = upd_legs = 0
    for i, r in df[is_parlay].iterrows():
        legs = _cr_parse_legs_json(r.get("legs_json"))
        changed = False
        for lg in legs:
            k = _cr_leg_key(lg.get("game_id"), lg.get("market"), lg.get("side"), lg.get("line"))
            if k in index:
                new_res = index[k]
                if (lg.get("result") or "").lower() != new_res:
                    lg["result"] = new_res
                    changed = True
                    upd_legs += 1
        if changed:
            df.at[i, "legs_json"] = _json.dumps(legs)
            upd_parlays += 1

    if upd_parlays > 0:
        df.to_csv(p, index=False)
    return (upd_parlays, upd_legs)

# Small UI control (safe to show anywhere on the Bet Log page)
with st.expander("Sync parlay legs with single-leg results"):
    st.caption("Writes leg-level 'result' into each parlay's legs_json by matching settled single-leg bets.")
    if st.button("Run parlay leg sync"):
        try:
            from pathlib import Path as _Path
            _bets = (_Path(st.session_state.get("EDGE_FINDER_ROOT") or ".") / "exports" / "bets_log.csv")
        except Exception:
            from pathlib import Path as _Path
            _bets = _Path(__file__).resolve().parents[3] / "exports" / "bets_log.csv"
        n_parlays, n_legs = sync_parlay_legs_into_json(str(_bets))
        st.success(f"Updated {n_parlays} parlay rows, {n_legs} leg results.")

